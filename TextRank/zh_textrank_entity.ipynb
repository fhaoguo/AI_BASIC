{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f3aa320",
   "metadata": {},
   "source": [
    "# 1 TextRank实体抽取类设计与实现\n",
    "---------------------------------------------------------------------\n",
    "### 1.1 基于窗口共现的TextRank类实现\n",
    "\n",
    "Equation: $WS(V_i)=(1 - d)\\frac{1}{N} + d \\sum_{V_j \\in In(V_i)} \\frac {W_{ji}}{\\sum_{V_k \\in Out(V_j)}} WS(V_j)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31516211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于窗口共现的TextRank类\n",
    "class WinBasedTextRank:\n",
    "    def __init__(self, double_word_list):\n",
    "        self.double_word_list = double_word_list\n",
    "        # 构建词表\n",
    "        self.all_words = self.__get_all_words()\n",
    "        # 键值对：词->窗口内词集\n",
    "        self.win_dict = self.__get_win_dict()\n",
    "        self.score_dict = self.__get_TextRank_score_dict()\n",
    "        \n",
    "    def __get_all_words(self):\n",
    "        words = []\n",
    "        for sent in self.double_word_list:\n",
    "            for word in sent:\n",
    "                words.append(word)\n",
    "        \n",
    "        return words\n",
    "    \n",
    "    def __get_win_dict(self):\n",
    "        win_dict = {}\n",
    "        for i in range(len(self.all_words)):\n",
    "            if self.all_words[i] not in win_dict.keys():\n",
    "                win_dict[self.all_words[i]] = set() \n",
    "            lindex = i - 5\n",
    "            if lindex < 0:\n",
    "                lindex = 0\n",
    "            rindex = i + 5 \n",
    "            if rindex > len(self.all_words) - 1:\n",
    "                rindex = len(self.all_words) - 1\n",
    "            for wd in self.all_words[lindex : rindex]:\n",
    "                win_dict[self.all_words[i]].add(wd)\n",
    "        \n",
    "        return win_dict\n",
    "    \n",
    "    def __get_TextRank_score_dict(self):\n",
    "        time = 0\n",
    "        score_dict = {w:0.99/len(self.all_words) for w in self.all_words}\n",
    "        while(time < 50):\n",
    "            for k, v in self.win_dict.items():\n",
    "                s = score_dict[k] / len(v)\n",
    "                score_dict[k] = 0\n",
    "                for i in v:\n",
    "                    score_dict[i] += 0.01*s\n",
    "            time += 1\n",
    "        \n",
    "        return score_dict    \n",
    "    \n",
    "    def get_TextRank_kv(self, text):\n",
    "        temp_dict = {}\n",
    "        for word in text:\n",
    "            if word in self.score_dict.keys():\n",
    "                temp_dict[word] = (self.score_dict[word])\n",
    "        values_list = sorted(temp_dict.items(), key=lambda temp_dict:temp_dict[1], reverse=True) #TextRank从大到小排序\n",
    "    \n",
    "        return values_list\n",
    "    \n",
    "    def get_TextRank_k(self, text):\n",
    "        values_list = self.get_TextRank_kv(text)\n",
    "        key_list = []\n",
    "        for key in values_list:\n",
    "            key_list.append(value[0])\n",
    "        \n",
    "        return (key_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dda9c9",
   "metadata": {},
   "source": [
    "### 1.2 基于FastText词向量计算相似度的TextRank实现\n",
    "### pagerank equation: $r_j = \\sum_{i \\rightarrow j} \\beta \\frac {r_i} {d_i} + (1 - \\beta) \\frac {1}{N}$\n",
    "##### FastText Model类定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76c5f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "import time\n",
    "\n",
    "class FTModel:\n",
    "    def __init__(self, model_path):\n",
    "        print('FastText word2vec model loading...', end='')\n",
    "        start = time.time()\n",
    "        self.model = FastText.load_fasttext_format(model_path)\n",
    "        end = time.time()\n",
    "        print('successfully, elapsed: %.2f s' % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1289e8f4",
   "metadata": {},
   "source": [
    "##### 加载FastText词向量模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ce9df3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText word2vec model loading..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21196/3929849022.py:8: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  self.model = FastText.load_fasttext_format(model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully, elapsed: 483.06 s\n"
     ]
    }
   ],
   "source": [
    "model_path = '../../../models/FastText/cc.zh.300.bin.gz'\n",
    "ft_model = FTModel(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd88245",
   "metadata": {},
   "source": [
    "##### FastText相似的TextRank类代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b81c8a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-18 02:45:51.944160: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-18 02:45:51.945306: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "class FTSimBasedTextRank:\n",
    "    def __init__(self, model, double_word_list):\n",
    "        self.ft_model = model\n",
    "        self.double_word_list = double_word_list\n",
    "        # 构建词表\n",
    "        self.all_words = self.__get_all_words()\n",
    "        self.word_embeddings = self.__get_embeddings()\n",
    "        self.word_sim_mat = self.__get_word_sim_mat()\n",
    " \n",
    "    def __get_all_words(self):\n",
    "        words = []\n",
    "        for sent in self.double_word_list:\n",
    "            for word in sent:\n",
    "                words.append(word)\n",
    "        \n",
    "        return words\n",
    "    \n",
    "    def __get_embeddings(self):\n",
    "        word_embeddings = {}\n",
    "        for word in self.all_words:\n",
    "            try:\n",
    "                # model.wv[word]存的就是这个word的词向量\n",
    "                word_embeddings[word] = self.ft_model.model.wv[word]\n",
    "            except KeyError:\n",
    "                continue\n",
    "        return word_embeddings\n",
    "    \n",
    "    def __get_word_sim_mat(self):\n",
    "        word_sim_vectors = []\n",
    "        for _, v in self.word_embeddings.items():\n",
    "            word_sim_vectors.append(v)\n",
    "        word_sim_mat = np.zeros([len(word_sim_vectors), len(word_sim_vectors)])\n",
    "        for i in range(len(word_sim_vectors)):\n",
    "            for j in range(len(word_sim_vectors)):\n",
    "                if i != j:\n",
    "                    word_sim_mat[i][j] = cosine_similarity(word_sim_vectors[i].reshape(1, 300), word_sim_vectors[j].reshape(1, 300))[0, 0]\n",
    "        return word_sim_mat\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        #fit_on_texts函数可以将输入的文本中的每个词编号，\n",
    "        #编号是根据词频的，词频越大，编号越小\n",
    "        tokenizer=Tokenizer()\n",
    "        tokenizer.fit_on_texts(self.double_word_list)\n",
    "        vocab = tokenizer.word_index  # 得到每个词的编号\n",
    "        return vocab\n",
    "\n",
    "    def get_TextRank_kv(self):\n",
    "        word_nx_graph = nx.from_numpy_array(self.word_sim_mat)\n",
    "        word_scores = nx.pagerank(word_nx_graph)\n",
    "        ranked_words = sorted(((word_scores[i], s) for i,s in enumerate(self.get_vocab())), reverse=True)\n",
    "        return ranked_words\n",
    "    \n",
    "    def get_TextRank_k(self):\n",
    "        ranked_words = get_TextRank_kv()\n",
    "        keys = []\n",
    "        for v, k in ranked_words:\n",
    "            keys.append(k)\n",
    "        return keys\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a0e3f7",
   "metadata": {},
   "source": [
    "# 2 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba2d124",
   "metadata": {},
   "source": [
    "### 2.1 爬取网上一篇文章并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "365833c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "爬取文本：\n",
      " 标题： 回答世界之问，习近平四个比喻发人深省 \n",
      "正文： ['\\u3000\\u3000原标题：第一观察 | 回答世界之问，习近平四个比喻发人深省', '\\u3000\\u3000五年前，1月17日，习近平主席在瑞士达沃斯出席世界经济论坛2017年年会，面对“世界怎么了、我们怎么办”的世界之问，纵论经济全球化走向，为迷茫困顿中的人们带来信心和力量。', '\\u3000\\u3000这一次，也是1月17日，习近平主席出席2022年世界经济论坛视频会议并发表演讲，面对“如何战胜疫情？如何建设疫后世界？”的又一世界之问，呼吁各方坚定信心、勇毅前行，为共创后疫情时代美好世界指明方向。', '\\u3000\\u3000从习近平主席演讲中提到的“大船”之喻、“大江”之喻、“蛋糕”之喻、“虎年”之喻中，我们感受着在“深刻而宏阔的时代之变”面前，中国领导人的历史和哲学思索，远见、自信与担当。', '\\u3000\\u3000', '\\u3000\\u3000“我们同在一条船上”——2018年11月，出席在太平洋探索者号邮轮上举行的亚太经合组织工商领导人峰会，习近平主席曾以“船”为喻，呼吁各方“同舟共济”，共同应对风险挑战。', '\\u3000\\u3000今天，习近平主席再次用到“船”的比喻。这次，他谈到的是“小船”和“大船”的关系。', '\\u3000\\u3000经历了持续延宕的新冠肺炎疫情，人类比以往更加感受到何为“同舟共济”。在一个“黑天鹅”和“灰犀牛”频频造访的世界，在各种可以预见的狂风暴雨和难以想象的惊涛骇浪面前，“小船”只会风雨飘摇，“巨舰”才能破浪前行。', '\\u3000\\u3000然而，并不是所有人都有这样的历史自觉。在疫情等全球性挑战面前，相互掣肘、无端“甩锅”者有之，贻误战机、干扰大局者有之，煽动仇恨偏见、进行围堵打压甚至对抗者有之……凡此种种，都无助于战胜共同面临的危机。', '\\u3000\\u3000“大船”之喻，正是人类命运共同体之喻。', '\\u3000\\u3000众人划桨开大船。', '\\u3000\\u3000于潮平海阔之时拉紧彼此，是瞭望者的历史远见；', '\\u3000\\u3000于风狂雨骤之时把握方向，是领航者的历史担当。', '\\u3000\\u3000', '\\u3000\\u3000“青山遮不住，毕竟东流去。”', '\\u3000\\u3000中国古人在对自然界的观察中，体悟着世界运行的规律和人生的哲理。', '\\u3000\\u3000世界大势，浩浩汤汤。“在历史前进的逻辑中前进、在时代发展的潮流中发展”，前提是正确认识、科学把握历史规律。', '\\u3000\\u3000在世界经济复苏面临诸多制约因素和不确定性加剧的形势下，“大江”之喻，显示出习近平主席对经济全球化大势的深刻把握。', '\\u3000\\u3000五年前在达沃斯，习近平主席纵论经济全球化，曾以“大海”比喻世界经济——', '\\u3000\\u3000“世界经济的大海，你要还是不要，都在那儿，是回避不了的”“让世界经济的大海退回到一个一个孤立的小湖泊、小河流，是不可能的，也是不符合历史潮流的”。', '\\u3000\\u3000“大海”，展示着经济全球化的广度与深度；', '\\u3000\\u3000“大江”，彰显着经济全球化的韧性与势能。', '\\u3000\\u3000登高壮观天地间，大江茫茫去不还。', '\\u3000\\u3000纵然有逆流、险滩，但“动力助其前行，阻力促其强大”，经济全球化方向从未改变、也不会改变。关键是要朝着正确方向，坚持拆墙而不筑墙、开放而不隔绝、融合而不脱钩，让世界经济活力充分迸发出来。', '\\u3000\\u3000习近平主席郑重宣示——', '\\u3000\\u3000“不论国际形势发生什么变化，中国都将高举改革开放的旗帜”“欢迎各种资本在中国合法依规经营，为中国发展发挥积极作用”“进一步融入区域和世界经济，努力实现互利共赢”……', '\\u3000\\u3000开放的中国，一直坚定地站在历史正确的一边。', '\\u3000\\u3000', '\\u3000\\u3000“共同富裕”正在成为国内外谈及中国经济和社会发展时的一个热词。', '\\u3000\\u3000在中国发展新的历史起点上，要推动人的全面发展、全体人民共同富裕取得更为明显的实质性进展，将采取何种方式？', '\\u3000\\u3000“蛋糕”之喻，形象地指明了中国实现共同富裕的路径，有助于国内外各界准确理解中国共同富裕内涵。', '\\u3000\\u3000五年前在达沃斯、日内瓦，习近平主席论及世界经济，也用了“蛋糕”的比喻——', '\\u3000\\u3000“当世界经济处于下行期的时候，全球经济‘蛋糕’不容易做大，甚至变小了”“既要做大蛋糕，更要分好蛋糕，着力解决公平公正问题”。', '\\u3000\\u3000彼时，针对经济全球化中出现的问题，以“蛋糕”之喻分析原因，指明的是均衡发展的路径；今天，围绕中国推进共同富裕，以“蛋糕”为喻，蕴涵的是高质量发展的深意。', '\\u3000\\u3000在习近平主席的宏阔视野中，中国与世界，人心相通，命运相连——', '\\u3000\\u3000指向“人类命运共同体”的“大船”之喻，指向人类前进正确方向的“大江”之喻，指向“以人民为中心”的“蛋糕”之喻，最终都是一个主题：人的全面发展。', '\\u3000\\u3000要实现人的全面发展，就要走高质量发展之路——', '\\u3000\\u3000谈及各方关心的中国经济走势，习近平主席在演讲中表示，“我们对中国经济发展前途充满信心”。在国内外经济环境变化带来巨大压力的形势下，对中国经济的信心，归根结底是对中国走高质量发展之路的信心。', '\\u3000\\u3000要实现人的全面发展，就要促进人与自然和谐共生——', '\\u3000\\u3000“发展经济不能对资源和生态环境竭泽而渔，生态环境保护也不是舍弃经济发展而缘木求鱼”。这是充满中国传统文化智慧的辩证法。加强污染防治、推动系统治理、实现碳达峰碳中和……推进生态文明建设的行动，归根结底是为了更好地建设人类文明。', '\\u3000\\u3000要实现人的全面发展，就不能让一个人、一个国家掉队——', '\\u3000\\u3000世界仍不太平，许多人还生活在贫困饥饿之中，疫情更使其雪上加霜。习近平主席在演讲中再提全球发展倡议，指出“不论遇到什么困难，我们都要坚持以人民为中心的发展思想”，展现出马克思主义政治家深厚的人民情怀和人类情怀。', '\\u3000\\u3000', '\\u3000\\u3000今天的人类，正在经历前所未有的时代之变；今天的世界，进入了新的动荡变革期。', '\\u3000\\u3000变局、乱局面前，勇气和力量至关重要。', '\\u3000\\u3000再过两周，中国农历虎年新春就要到来。在中国文化中，虎象征着勇敢和力量。习近平主席以“虎”为喻，把中国传统文化和世界形势变化结合起来，为全人类克服种种艰难险阻提供了宝贵的中华文明滋养。', '\\u3000\\u3000“世界总是在矛盾运动中发展的，没有矛盾就没有世界。纵观历史，人类正是在战胜一次次考验中成长、在克服一场场危机中发展。”历史发展有其规律，但人在其中不是完全消极被动的。', '\\u3000\\u3000“虎年”之喻，体现出新时代中国共产党人在深刻把握历史大势和历史规律的同时，所具有的勇于战胜一切艰难险阻、推动人类文明进步的历史主动精神。', '\\u3000\\u3000五年前，在达沃斯，习近平主席说：“遇到了困难，不要埋怨自己，不要指责他人，不要放弃信心，不要逃避责任，而是要一起来战胜困难。历史是勇敢者创造的。”', '\\u3000\\u3000今天，出席“云上”会议，习近平主席以“生龙活虎”“龙腾虎跃”“如虎添翼”“虎虎生威”，鼓舞和激励全世界坚定信心、勇毅前行。巨变中的世界，如今人类最需要的就是这份信心。', '\\u3000\\u3000监制：赵承', '\\u3000\\u3000策划：霍小光', '\\u3000\\u3000主笔：杨依军', '\\u3000\\u3000统筹：刘华、王绚', '\\u3000\\u3000视觉 | 编辑：杨文荣、王秋韵、唐兴', '\\u3000\\u3000新华社国内部制作', '\\u3000\\u3000新华社第一工作室出品', '责任编辑：胡越 SN231']\n",
      "回答世界之问，习近平四个比喻发人深省　　原标题：第一观察 | 回答世界之问，习近平四个比喻发人深省　　五年前，1月17日，习近平主席在瑞士达沃斯出席世界经济论坛2017年年会，面对“世界怎么了、我们怎么办”的世界之问，纵论经济全球化走向，为迷茫困顿中的人们带来信心和力量。　　这一次，也是1月17日，习近平主席出席2022年世界经济论坛视频会议并发表演讲，面对“如何战胜疫情？如何建设疫后世界？”的又一世界之问，呼吁各方坚定信心、勇毅前行，为共创后疫情时代美好世界指明方向。　　从习近平主席演讲中提到的“大船”之喻、“大江”之喻、“蛋糕”之喻、“虎年”之喻中，我们感受着在“深刻而宏阔的时代之变”面前，中国领导人的历史和哲学思索，远见、自信与担当。　　　　“我们同在一条船上”——2018年11月，出席在太平洋探索者号邮轮上举行的亚太经合组织工商领导人峰会，习近平主席曾以“船”为喻，呼吁各方“同舟共济”，共同应对风险挑战。　　今天，习近平主席再次用到“船”的比喻。这次，他谈到的是“小船”和“大船”的关系。　　经历了持续延宕的新冠肺炎疫情，人类比以往更加感受到何为“同舟共济”。在一个“黑天鹅”和“灰犀牛”频频造访的世界，在各种可以预见的狂风暴雨和难以想象的惊涛骇浪面前，“小船”只会风雨飘摇，“巨舰”才能破浪前行。　　然而，并不是所有人都有这样的历史自觉。在疫情等全球性挑战面前，相互掣肘、无端“甩锅”者有之，贻误战机、干扰大局者有之，煽动仇恨偏见、进行围堵打压甚至对抗者有之……凡此种种，都无助于战胜共同面临的危机。　　“大船”之喻，正是人类命运共同体之喻。　　众人划桨开大船。　　于潮平海阔之时拉紧彼此，是瞭望者的历史远见；　　于风狂雨骤之时把握方向，是领航者的历史担当。　　　　“青山遮不住，毕竟东流去。”　　中国古人在对自然界的观察中，体悟着世界运行的规律和人生的哲理。　　世界大势，浩浩汤汤。“在历史前进的逻辑中前进、在时代发展的潮流中发展”，前提是正确认识、科学把握历史规律。　　在世界经济复苏面临诸多制约因素和不确定性加剧的形势下，“大江”之喻，显示出习近平主席对经济全球化大势的深刻把握。　　五年前在达沃斯，习近平主席纵论经济全球化，曾以“大海”比喻世界经济——　　“世界经济的大海，你要还是不要，都在那儿，是回避不了的”“让世界经济的大海退回到一个一个孤立的小湖泊、小河流，是不可能的，也是不符合历史潮流的”。　　“大海”，展示着经济全球化的广度与深度；　　“大江”，彰显着经济全球化的韧性与势能。　　登高壮观天地间，大江茫茫去不还。　　纵然有逆流、险滩，但“动力助其前行，阻力促其强大”，经济全球化方向从未改变、也不会改变。关键是要朝着正确方向，坚持拆墙而不筑墙、开放而不隔绝、融合而不脱钩，让世界经济活力充分迸发出来。　　习近平主席郑重宣示——　　“不论国际形势发生什么变化，中国都将高举改革开放的旗帜”“欢迎各种资本在中国合法依规经营，为中国发展发挥积极作用”“进一步融入区域和世界经济，努力实现互利共赢”……　　开放的中国，一直坚定地站在历史正确的一边。　　　　“共同富裕”正在成为国内外谈及中国经济和社会发展时的一个热词。　　在中国发展新的历史起点上，要推动人的全面发展、全体人民共同富裕取得更为明显的实质性进展，将采取何种方式？　　“蛋糕”之喻，形象地指明了中国实现共同富裕的路径，有助于国内外各界准确理解中国共同富裕内涵。　　五年前在达沃斯、日内瓦，习近平主席论及世界经济，也用了“蛋糕”的比喻——　　“当世界经济处于下行期的时候，全球经济‘蛋糕’不容易做大，甚至变小了”“既要做大蛋糕，更要分好蛋糕，着力解决公平公正问题”。　　彼时，针对经济全球化中出现的问题，以“蛋糕”之喻分析原因，指明的是均衡发展的路径；今天，围绕中国推进共同富裕，以“蛋糕”为喻，蕴涵的是高质量发展的深意。　　在习近平主席的宏阔视野中，中国与世界，人心相通，命运相连——　　指向“人类命运共同体”的“大船”之喻，指向人类前进正确方向的“大江”之喻，指向“以人民为中心”的“蛋糕”之喻，最终都是一个主题：人的全面发展。　　要实现人的全面发展，就要走高质量发展之路——　　谈及各方关心的中国经济走势，习近平主席在演讲中表示，“我们对中国经济发展前途充满信心”。在国内外经济环境变化带来巨大压力的形势下，对中国经济的信心，归根结底是对中国走高质量发展之路的信心。　　要实现人的全面发展，就要促进人与自然和谐共生——　　“发展经济不能对资源和生态环境竭泽而渔，生态环境保护也不是舍弃经济发展而缘木求鱼”。这是充满中国传统文化智慧的辩证法。加强污染防治、推动系统治理、实现碳达峰碳中和……推进生态文明建设的行动，归根结底是为了更好地建设人类文明。　　要实现人的全面发展，就不能让一个人、一个国家掉队——　　世界仍不太平，许多人还生活在贫困饥饿之中，疫情更使其雪上加霜。习近平主席在演讲中再提全球发展倡议，指出“不论遇到什么困难，我们都要坚持以人民为中心的发展思想”，展现出马克思主义政治家深厚的人民情怀和人类情怀。　　　　今天的人类，正在经历前所未有的时代之变；今天的世界，进入了新的动荡变革期。　　变局、乱局面前，勇气和力量至关重要。　　再过两周，中国农历虎年新春就要到来。在中国文化中，虎象征着勇敢和力量。习近平主席以“虎”为喻，把中国传统文化和世界形势变化结合起来，为全人类克服种种艰难险阻提供了宝贵的中华文明滋养。　　“世界总是在矛盾运动中发展的，没有矛盾就没有世界。纵观历史，人类正是在战胜一次次考验中成长、在克服一场场危机中发展。”历史发展有其规律，但人在其中不是完全消极被动的。　　“虎年”之喻，体现出新时代中国共产党人在深刻把握历史大势和历史规律的同时，所具有的勇于战胜一切艰难险阻、推动人类文明进步的历史主动精神。　　五年前，在达沃斯，习近平主席说：“遇到了困难，不要埋怨自己，不要指责他人，不要放弃信心，不要逃避责任，而是要一起来战胜困难。历史是勇敢者创造的。”　　今天，出席“云上”会议，习近平主席以“生龙活虎”“龙腾虎跃”“如虎添翼”“虎虎生威”，鼓舞和激励全世界坚定信心、勇毅前行。巨变中的世界，如今人类最需要的就是这份信心。　　监制：赵承　　策划：霍小光　　主笔：杨依军　　统筹：刘华、王绚　　视觉 | 编辑：杨文荣、王秋韵、唐兴　　新华社国内部制作　　新华社第一工作室出品\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "etree=html.etree\n",
    "#url='http://news.cri.cn/gb/27824/2013/01/28/6611s4005015.htm'\n",
    "url='https://news.sina.com.cn/gov/xlxw/2022-01-18/doc-ikyakumy1103837.shtml'\n",
    "data=requests.get(url)\n",
    "data.encoding='utf-8'\n",
    "#print(data)\n",
    "s=etree.HTML(data.text)\n",
    "# text1=s.xpath('//div[@id=\"ccontent\"]/p/text()')#得到的文本是一个列表，里面有6项，代表6个自然段\n",
    "# title=s.xpath('//div[@class=\"news_ts_tit\"]/text()')[0].strip()#[0]是取标题的第一项，trip()去掉首尾空格\n",
    "text1=s.xpath('//div[@id=\"article\"]/p/text()')#得到的文本是一个列表，里面有6项，代表6个自然段\n",
    "title=s.xpath('//h1[@class=\"main-title\"]/text()')[0].strip()#[0]是取标题的第一项，trip()去掉首尾空格\n",
    "print(\"爬取文本：\\n\",\"标题：\",title,\"\\n正文：\",text1)\n",
    "text=title\n",
    "\n",
    "# 将得到的文本写入文件\n",
    "for i in range(0,len(text1)-1):\n",
    "   text += text1[i]\n",
    "sentence_list=[]\n",
    "print(text)\n",
    "title='dat/' + title + '.txt'\n",
    "with open(title, 'w', encoding='utf-8') as f:\n",
    "   f.writelines(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c92ddb",
   "metadata": {},
   "source": [
    "### 2.2 打开文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98bcad1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前10个句子为：\n",
      "\n",
      "['回答世界之问，习近平四个比喻发人深省\\u3000\\u3000原标题：第一观察 | 回答世界之问，习近平四个比喻发人深省\\u3000\\u3000五年前，1月17日，习近平主席在瑞士达沃斯出席世界经济论坛2017年年会，面对“世界怎么了、我们怎么办”的世界之问，纵论经济全球化走向，为迷茫困顿中的人们带来信心和力量', '这一次，也是1月17日，习近平主席出席2022年世界经济论坛视频会议并发表演讲，面对“如何战胜疫情', '如何建设疫后世界', '”的又一世界之问，呼吁各方坚定信心、勇毅前行，为共创后疫情时代美好世界指明方向', '从习近平主席演讲中提到的“大船”之喻、“大江”之喻、“蛋糕”之喻、“虎年”之喻中，我们感受着在“深刻而宏阔的时代之变”面前，中国领导人的历史和哲学思索，远见、自信与担当', '“我们同在一条船上”——2018年11月，出席在太平洋探索者号邮轮上举行的亚太经合组织工商领导人峰会，习近平主席曾以“船”为喻，呼吁各方“同舟共济”，共同应对风险挑战', '今天，习近平主席再次用到“船”的比喻', '这次，他谈到的是“小船”和“大船”的关系', '经历了持续延宕的新冠肺炎疫情，人类比以往更加感受到何为“同舟共济”', '在一个“黑天鹅”和“灰犀牛”频频造访的世界，在各种可以预见的狂风暴雨和难以想象的惊涛骇浪面前，“小船”只会风雨飘摇，“巨舰”才能破浪前行']\n",
      "句子总数： 57\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re,jieba.posseg as pseg\n",
    "from itertools import chain\n",
    "\n",
    "#打开文件\n",
    "sentences_list = []\n",
    "file_path=title\n",
    "fp = open(file_path,'r',encoding=\"utf8\")\n",
    "for line in fp.readlines():\n",
    "        if line.strip():\n",
    "            # 把元素按照[。！；？]进行分隔，得到句子。\n",
    "            line_split = re.split(r'[。！；？]',line.strip())\n",
    "            # [。！；？]这些符号也会划分出来，把它们去掉。\n",
    "            line_split = [line.strip() for line in line_split if line.strip() not in ['。','！','？','；'] and len(line.strip())>1]\n",
    "            sentences_list.append(line_split)\n",
    "sentences_list = list(chain.from_iterable(sentences_list))\n",
    "print(\"前10个句子为：\\n\")\n",
    "print(sentences_list[:10])\n",
    "print(\"句子总数：\", len(sentences_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53023fc8",
   "metadata": {},
   "source": [
    "### 2.3 分词、停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c3d8daa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一共有 57 个句子。\n",
      "\n",
      "前10个句子分词后的结果为：\n",
      " [['回答', '世界', '问', '习近平', '四个', '比喻', '发人深省', '原', '标题', '第一', '观察', '回答', '世界', '问', '习近平', '四个', '比喻', '发人深省', '五年', '前月日', '习近平', '主席', '瑞士', '达沃斯', '出席', '世界', '经济', '论坛', '年', '年', '会', '面对', '世界', '世界', '问', '纵论', '经济', '全球化', '走向', '迷茫', '困顿', '中', '人们', '带来', '信心', '力量'], ['一次', '月', '日', '习近平', '主席', '出席', '年', '世界', '经济', '论坛', '视频会议', '发表', '演讲', '面对', '战胜', '疫情'], ['建设', '疫后', '世界'], ['世界', '问', '呼吁', '各方', '坚定信心', '勇毅', '前', '行为', '共创', '后', '疫情', '时代', '美好世界', '指明方向'], ['习近平', '主席', '演讲', '中', '提到', '大船', '喻', '大江', '喻', '蛋糕', '喻', '虎年', '喻', '中', '感受', '深刻', '宏阔', '时代', '变', '面前', '中国', '领导人', '历史', '哲学', '思索', '远见', '自信', '担当'], ['一条', '船上', '年', '月', '出席', '太平洋', '探索者', '号', '邮轮', '上', '举行', '亚太经合组织', '工商', '领导人', '峰会', '习近平', '主席', '曾', '船', '喻', '呼吁', '各方', '同舟共济', '共同', '应对', '风险', '挑战'], ['今天', '习近平', '主席', '再次', '用到', '船', '比喻'], ['这次', '谈到', '小船', '大船', '关系'], ['经历', '持续', '延宕', '新冠', '肺炎', '疫情', '人类', '以往', '更加', '感受', '何为', '同舟共济'], ['一个', '黑天鹅', '灰', '犀牛', '频频', '造访', '世界', '预见', '狂风暴雨', '难以想象', '惊涛骇浪', '面前', '小', '船只', '会', '风雨飘摇', '巨舰', '才能', '破浪', '前行']]\n",
      "\n",
      "数据预处理后句子的数量不变！\n"
     ]
    }
   ],
   "source": [
    "# 加载停用词\n",
    "stoplist= [word.strip() for word in open('stopwords.txt',encoding='utf-8').readlines()]\n",
    "# print(stoplist)\n",
    "word_flag = {}\n",
    "\n",
    "# 对句子进行分词\n",
    "def seg_depart(sentence):\n",
    "    # 去掉非汉字字符\n",
    "    sentence = re.sub(r'[^\\u4e00-\\u9fa5]+','',sentence)\n",
    "    sentence_depart = pseg.cut(sentence.strip())\n",
    "    word_list = []\n",
    "    for word,flag in sentence_depart:\n",
    "        if word not in stoplist:\n",
    "            word_list.append(word)\n",
    "            word_flag[word] = flag\n",
    "    # 如果句子整个被过滤掉了，如：'02-2717:56'被过滤，那就返回[],保持句子的数量不变\n",
    "    return word_list\n",
    "\n",
    "sentence_word_list = []\n",
    "for sentence in sentences_list:\n",
    "    line_seg = seg_depart(sentence)\n",
    "    sentence_word_list.append(line_seg)\n",
    "print(\"一共有\",len(sentences_list),'个句子。\\n')\n",
    "print(\"前10个句子分词后的结果为：\\n\",sentence_word_list[:10])\n",
    "\n",
    "# 保证处理后句子的数量不变，我们后面才好根据textrank值取出未处理之前的句子作为摘要。\n",
    "if len(sentences_list) == len(sentence_word_list):\n",
    "    print(\"\\n数据预处理后句子的数量不变！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1077379f",
   "metadata": {},
   "source": [
    "# 3 实体抽取\n",
    "### 3.1 窗口共现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "288181d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \t 2 \t ('习近平', 3.3827774991418443e-121) \t nrfg\n",
      "2 \t 13 \t ('喻', 1.1171176442893095e-121) \t nr\n",
      "3 \t 24 \t ('中国', 7.611595277588015e-122) \t ns\n",
      "4 \t 27 \t ('达沃斯', 6.481323485700361e-122) \t nr\n",
      "5 \t 28 \t ('五年', 6.453631707698266e-122) \t t\n",
      "6 \t 44 \t ('前月日', 4.957337940159634e-122) \t t\n",
      "7 \t 46 \t ('黑天鹅', 4.5767379093499387e-122) \t nz\n",
      "8 \t 47 \t ('瑞士', 4.5331473754738465e-122) \t ns\n",
      "9 \t 49 \t ('太平洋', 4.3306608142406035e-122) \t ns\n",
      "10 \t 51 \t ('今天', 4.1628399353490984e-122) \t t\n",
      "11 \t 62 \t ('探索者', 3.6517119249845045e-122) \t nr\n",
      "12 \t 65 \t ('以往', 3.568954650545539e-122) \t t\n",
      "13 \t 72 \t ('虎年', 3.212775901700971e-122) \t t\n",
      "14 \t 76 \t ('大江', 3.124390260632934e-122) \t ns\n",
      "15 \t 86 \t ('船上', 2.662247718952779e-122) \t s\n",
      "16 \t 96 \t ('大海', 2.2731492066608617e-122) \t ns\n",
      "17 \t 112 \t ('亚太经合组织', 1.5970845099530778e-122) \t nt\n",
      "18 \t 128 \t ('时', 1.0022870908674213e-122) \t ng\n",
      "19 \t 133 \t ('湖泊', 8.747212210813152e-123) \t ns\n",
      "20 \t 150 \t ('青山', 6.452771904432765e-123) \t ns\n",
      "21 \t 156 \t ('广度', 5.924341191058235e-123) \t ns\n",
      "22 \t 157 \t ('潮平海', 5.8661753782126756e-123) \t ns\n",
      "23 \t 158 \t ('深度', 5.861379100101569e-123) \t ns\n",
      "24 \t 182 \t ('改革开放', 4.0304469032410617e-123) \t nz\n",
      "25 \t 193 \t ('筑墙', 3.683997274643997e-123) \t nz\n",
      "26 \t 209 \t ('美好世界', 2.9811841465543483e-123) \t nz\n",
      "27 \t 219 \t ('共同富裕', 2.356531016891228e-123) \t nz\n",
      "28 \t 251 \t ('努力实现', 1.0862459225397706e-123) \t nr\n",
      "29 \t 283 \t ('前在', 6.518407061908698e-124) \t t\n",
      "30 \t 285 \t ('国内外', 6.458136973559344e-124) \t s\n",
      "31 \t 286 \t ('太平', 6.420478095100654e-124) \t ns\n",
      "32 \t 309 \t ('正在', 4.710867822463371e-124) \t t\n",
      "33 \t 321 \t ('龙腾虎跃', 3.851115165956912e-124) \t nr\n",
      "34 \t 337 \t ('宝贵', 3.0300924445792183e-124) \t nr\n",
      "35 \t 350 \t ('中华文明', 2.500144912960317e-124) \t ns\n",
      "36 \t 354 \t ('如今', 2.18081023549452e-124) \t t\n",
      "37 \t 361 \t ('巨变', 2.0576736625846596e-124) \t nz\n",
      "38 \t 375 \t ('新春', 1.759638028696297e-124) \t ns\n",
      "39 \t 387 \t ('公正', 1.5386235966065585e-124) \t nr\n",
      "40 \t 389 \t ('中虎', 1.470742672858447e-124) \t nz\n",
      "41 \t 391 \t ('日内瓦', 1.4517094194019469e-124) \t ns\n",
      "42 \t 398 \t ('智慧', 1.3414516704756382e-124) \t nr\n",
      "43 \t 426 \t ('赵承', 8.800524610103131e-125) \t nr\n",
      "44 \t 439 \t ('文明', 7.517620811624292e-125) \t nr\n",
      "45 \t 448 \t ('碳达峰', 6.208273818727152e-125) \t nz\n",
      "46 \t 452 \t ('霍小光', 5.724757934959186e-125) \t nr\n"
     ]
    }
   ],
   "source": [
    "word_list = []\n",
    "for sent in sentence_word_list:\n",
    "    for word in sent:\n",
    "        word_list.append(word)\n",
    "        \n",
    "win_tr = WinBasedTextRank(sentence_word_list)\n",
    "win_kvs = win_tr.get_TextRank_kv(word_list)\n",
    "\n",
    "i = 0\n",
    "cnt = 0\n",
    "for k, v in win_kvs[:int(len(win_kvs)*.95)]:\n",
    "    fg = word_flag[k]\n",
    "    if (fg.startswith('n') and len(fg)>1) or fg.startswith('t') or fg.startswith('s'):\n",
    "        cnt += 1\n",
    "        print(cnt, \"\\t\", i+1, \"\\t\", (k,v), '\\t', fg)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3153b6f",
   "metadata": {},
   "source": [
    "### 3.2 FastText词向量相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "710c26e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \t 24 \t ('宝贵', 0.002742921409241907) \t nr\n",
      "2 \t 29 \t ('五年', 0.0027293775653264925) \t t\n",
      "3 \t 51 \t ('新春', 0.0026291983065427817) \t ns\n",
      "4 \t 74 \t ('改革开放', 0.0025576802178185433) \t nz\n",
      "5 \t 99 \t ('深度', 0.002475588789583414) \t ns\n",
      "6 \t 112 \t ('努力实现', 0.002442140037452893) \t nr\n",
      "7 \t 122 \t ('新华社', 0.0024129291292472425) \t nt\n",
      "8 \t 143 \t ('以往', 0.00236739061339443) \t t\n",
      "9 \t 150 \t ('杨依军', 0.0023492412172824084) \t nr\n",
      "10 \t 159 \t ('船上', 0.0023332539441397403) \t s\n",
      "11 \t 164 \t ('筑墙', 0.0023223507902135126) \t nz\n",
      "12 \t 166 \t ('大海', 0.0023172749795044145) \t ns\n",
      "13 \t 173 \t ('时', 0.002306819914010548) \t ng\n",
      "14 \t 189 \t ('中虎', 0.0022818342755228176) \t nz\n",
      "15 \t 196 \t ('日内瓦', 0.0022693665588778794) \t ns\n",
      "16 \t 197 \t ('中国共产党', 0.0022669348725534925) \t nt\n",
      "17 \t 204 \t ('赵承', 0.002256275526432507) \t nr\n",
      "18 \t 207 \t ('智慧', 0.002255556223351494) \t nr\n",
      "19 \t 208 \t ('碳达峰', 0.0022542864614083953) \t nz\n",
      "20 \t 212 \t ('巨变', 0.002250422305440314) \t nz\n",
      "21 \t 220 \t ('大江', 0.002236526919691958) \t ns\n",
      "22 \t 222 \t ('黑天鹅', 0.0022346143364660597) \t nz\n",
      "23 \t 234 \t ('中华文明', 0.002208176642338083) \t ns\n",
      "24 \t 240 \t ('今天', 0.002196016973207245) \t t\n",
      "25 \t 244 \t ('如今', 0.0021873385135497683) \t t\n",
      "26 \t 253 \t ('瑞士', 0.0021665414885201255) \t ns\n",
      "27 \t 254 \t ('前在', 0.0021658497168749753) \t t\n",
      "28 \t 263 \t ('美好世界', 0.0021260422361240416) \t nz\n",
      "29 \t 268 \t ('中国', 0.0021112996883398683) \t ns\n",
      "30 \t 284 \t ('潮平海', 0.0020705052095159986) \t ns\n",
      "31 \t 290 \t ('探索者', 0.002050848639256102) \t nr\n",
      "32 \t 305 \t ('共同富裕', 0.0020198923922321627) \t nz\n",
      "33 \t 314 \t ('习近平', 0.001983253308073484) \t nrfg\n",
      "34 \t 318 \t ('文明', 0.0019636108618007494) \t nr\n",
      "35 \t 320 \t ('王秋韵', 0.0019598677241633184) \t nr\n",
      "36 \t 321 \t ('唐兴', 0.001959674450376885) \t nr\n",
      "37 \t 338 \t ('前月日', 0.001938727954818321) \t t\n",
      "38 \t 341 \t ('达沃斯', 0.001936872547816818) \t nr\n",
      "39 \t 347 \t ('湖泊', 0.001919622351164498) \t ns\n",
      "40 \t 364 \t ('霍小光', 0.0018631380652643637) \t nr\n",
      "41 \t 367 \t ('太平洋', 0.0018585049454701295) \t ns\n",
      "42 \t 370 \t ('龙腾虎跃', 0.001840787939811052) \t nr\n",
      "43 \t 382 \t ('公正', 0.001799364544120426) \t nr\n",
      "44 \t 390 \t ('太平', 0.0017573217154072788) \t ns\n",
      "45 \t 393 \t ('广度', 0.0017388419492046325) \t ns\n",
      "46 \t 402 \t ('亚太经合组织', 0.0016832766248415074) \t nt\n",
      "47 \t 412 \t ('正在', 0.0016256474509927675) \t t\n",
      "48 \t 421 \t ('喻', 0.0015751182241149188) \t nr\n",
      "49 \t 425 \t ('青山', 0.001549113510841338) \t ns\n",
      "50 \t 430 \t ('国内外', 0.0015273368323536463) \t s\n",
      "51 \t 439 \t ('刘华王', 0.0014446849374016415) \t nr\n",
      "52 \t 445 \t ('国内部', 0.001345601970913253) \t nt\n",
      "53 \t 463 \t ('杨文荣', 0.0006215002963926477) \t nr\n"
     ]
    }
   ],
   "source": [
    "ftsim_tr = FTSimBasedTextRank(ft_model, sentence_word_list)\n",
    "ftsim_kvs = ftsim_tr.get_TextRank_kv()\n",
    "\n",
    "i = 0\n",
    "cnt = 0\n",
    "for v, k in ftsim_kvs[:int(len(ftsim_kvs)*.95)]:\n",
    "    fg = word_flag[k]\n",
    "    if (fg.startswith('n') and len(fg)>1) or fg.startswith('t') or fg.startswith('s'):\n",
    "        cnt += 1\n",
    "        print(cnt, \"\\t\", i+1, \"\\t\", (k,v), '\\t', fg)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eefb53",
   "metadata": {},
   "source": [
    "### 3.3 集成实体抽取结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d3d6aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_entities = []\n",
    "for k, v in win_kvs[:int(len(win_kvs)*.95)]:\n",
    "    fg = word_flag[k]\n",
    "    if (fg.startswith('n') and len(fg)>1) or fg.startswith('t') or fg.startswith('s'):\n",
    "        win_entities.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "52a6c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = []\n",
    "for v, k in ftsim_kvs[:int(len(ftsim_kvs)*.95)]:\n",
    "    fg = word_flag[k]\n",
    "    if (fg.startswith('n') and len(fg)>1) or fg.startswith('t') or fg.startswith('s'):\n",
    "        if k in win_entities:\n",
    "            entities.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7ce3ddc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['宝贵',\n",
       " '五年',\n",
       " '新春',\n",
       " '改革开放',\n",
       " '深度',\n",
       " '努力实现',\n",
       " '以往',\n",
       " '船上',\n",
       " '筑墙',\n",
       " '大海',\n",
       " '时',\n",
       " '中虎',\n",
       " '日内瓦',\n",
       " '赵承',\n",
       " '智慧',\n",
       " '碳达峰',\n",
       " '巨变',\n",
       " '大江',\n",
       " '黑天鹅',\n",
       " '中华文明',\n",
       " '今天',\n",
       " '如今',\n",
       " '瑞士',\n",
       " '前在',\n",
       " '美好世界',\n",
       " '中国',\n",
       " '潮平海',\n",
       " '探索者',\n",
       " '共同富裕',\n",
       " '习近平',\n",
       " '文明',\n",
       " '前月日',\n",
       " '达沃斯',\n",
       " '湖泊',\n",
       " '霍小光',\n",
       " '太平洋',\n",
       " '龙腾虎跃',\n",
       " '公正',\n",
       " '太平',\n",
       " '广度',\n",
       " '亚太经合组织',\n",
       " '正在',\n",
       " '喻',\n",
       " '青山',\n",
       " '国内外']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e for e in entities]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997a5ade",
   "metadata": {},
   "source": [
    "### 3.4 确定实体类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "375cce63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>ch_pos</th>\n",
       "      <th>cate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nr</td>\n",
       "      <td>人名</td>\n",
       "      <td>人物</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nr1</td>\n",
       "      <td>汉语姓氏</td>\n",
       "      <td>人物</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nr2</td>\n",
       "      <td>汉语名字</td>\n",
       "      <td>人物</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nrj</td>\n",
       "      <td>日语人名</td>\n",
       "      <td>人物</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nrf</td>\n",
       "      <td>音译人名</td>\n",
       "      <td>人物</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ns</td>\n",
       "      <td>地名</td>\n",
       "      <td>地址</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nsf</td>\n",
       "      <td>音译地名</td>\n",
       "      <td>地址</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nt</td>\n",
       "      <td>机构团体名</td>\n",
       "      <td>机构团体</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nz</td>\n",
       "      <td>其它专名</td>\n",
       "      <td>其它</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>t</td>\n",
       "      <td>时间词</td>\n",
       "      <td>时间</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tg</td>\n",
       "      <td>时间词性语素</td>\n",
       "      <td>时间</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>s</td>\n",
       "      <td>处所词</td>\n",
       "      <td>处所</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pos  ch_pos  cate\n",
       "0    nr      人名    人物\n",
       "1   nr1    汉语姓氏    人物\n",
       "2   nr2    汉语名字    人物\n",
       "3   nrj    日语人名    人物\n",
       "4   nrf    音译人名    人物\n",
       "5    ns      地名    地址\n",
       "6   nsf    音译地名    地址\n",
       "7    nt   机构团体名  机构团体\n",
       "8    nz    其它专名    其它\n",
       "9     t     时间词    时间\n",
       "10   tg  时间词性语素    时间\n",
       "11    s     处所词    处所"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "e_cate = pd.read_csv(\"./entity_cate.csv\", encoding='utf-8', header=0)\n",
    "e_cate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7e431c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "实例\t\t概念\n",
      "-------------------------\n",
      "宝贵 \t\t 人物\n",
      "五年 \t\t 时间\n",
      "新春 \t\t 地址\n",
      "改革开放 \t\t 其它\n",
      "深度 \t\t 地址\n",
      "努力实现 \t\t 人物\n",
      "以往 \t\t 时间\n",
      "船上 \t\t 处所\n",
      "筑墙 \t\t 其它\n",
      "大海 \t\t 地址\n",
      "时 \t\t 未知概念\n",
      "中虎 \t\t 其它\n",
      "日内瓦 \t\t 地址\n",
      "赵承 \t\t 人物\n",
      "智慧 \t\t 人物\n",
      "碳达峰 \t\t 其它\n",
      "巨变 \t\t 其它\n",
      "大江 \t\t 地址\n",
      "黑天鹅 \t\t 其它\n",
      "中华文明 \t\t 地址\n",
      "今天 \t\t 时间\n",
      "如今 \t\t 时间\n",
      "瑞士 \t\t 地址\n",
      "前在 \t\t 时间\n",
      "美好世界 \t\t 其它\n",
      "中国 \t\t 地址\n",
      "潮平海 \t\t 地址\n",
      "探索者 \t\t 人物\n",
      "共同富裕 \t\t 其它\n",
      "习近平 \t\t 未知概念\n",
      "文明 \t\t 人物\n",
      "前月日 \t\t 时间\n",
      "达沃斯 \t\t 人物\n",
      "湖泊 \t\t 地址\n",
      "霍小光 \t\t 人物\n",
      "太平洋 \t\t 地址\n",
      "龙腾虎跃 \t\t 人物\n",
      "公正 \t\t 人物\n",
      "太平 \t\t 地址\n",
      "广度 \t\t 地址\n",
      "亚太经合组织 \t\t 机构团体\n",
      "正在 \t\t 时间\n",
      "喻 \t\t 人物\n",
      "青山 \t\t 地址\n",
      "国内外 \t\t 处所\n"
     ]
    }
   ],
   "source": [
    "print(\"实例\\t\\t概念\")\n",
    "print(\"-------------------------\")\n",
    "for e in entities:\n",
    "    cate = []\n",
    "    cate = list(e_cate[e_cate['pos'] == word_flag[e]]['cate'])\n",
    "    if len(cate) == 0:\n",
    "        cate = [\"未知概念\"]\n",
    "    print(e, '\\t\\t', cate[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6443576",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
