{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B28sUtgRTsA-",
        "outputId": "d5a4503b-ccf2-45e5-b08a-2cde2bea85fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "训练集大小: torch.Size([60000, 28, 28])\n",
            "训练集标签个数: torch.Size([60000])\n",
            "测试集大小: torch.Size([10000, 28, 28])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:62: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:52: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:67: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# 超参数\n",
        "EPOCH = 1\n",
        "BATCH_SIZE = 64\n",
        "LR = 0.001\n",
        "\n",
        "my_tf = transforms.Compose([\n",
        "                            transforms.Resize(224), \n",
        "                            transforms.ToTensor()])\n",
        "\n",
        "# 下载fashion-mnist\n",
        "train_data = datasets.FashionMNIST(root='/nas/fashion-mnist/', train=True, transform=my_tf, download=True)\n",
        "test_data = datasets.FashionMNIST(root='/nas/fashion-mnist/', train=False, transform=my_tf, download=True)\n",
        "\n",
        "print('训练集大小:', train_data.train_data.size())\n",
        "print('训练集标签个数:', train_data.train_labels.size())\n",
        "print('测试集大小:', test_data.test_data.size())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(train_data.train_data[0].numpy(), cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "TQpCtp26iFjS",
        "outputId": "d07989a0-0934-4f83-8c13-32ef69f69e65"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:62: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f5ca4dc95d0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 使用DataLoader进行分批\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    # 原始图是灰度图，1个通道，现在需要转化为3个通道\n",
        "    self.conv = nn.Conv2d(1, 3, kernel_size=1)\n",
        "    self.vgg16 = torchvision.models.vgg16(pretrained=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = self.vgg16(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "k8vSYByjjvpd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载模型\n",
        "model = Net()\n",
        "# 定义损失\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# 定义优化器\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# 设置设备\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# 训练\n",
        "for epoch in range(EPOCH):\n",
        "  for i, data in enumerate(train_loader):\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    # 前向传播\n",
        "    outputs = model(inputs)\n",
        "    # 计算损失\n",
        "    loss = criterion(outputs, labels)\n",
        "    print('iteration{}, loss:{:.4f}'.format(i+1, loss.item()))\n",
        "    # 清空上一轮梯度\n",
        "    optimizer.zero_grad()\n",
        "    # 反向传播\n",
        "    loss.backward()\n",
        "    # 参数更新呢\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1uUMc5plABd",
        "outputId": "8f6e8fcd-64e7-40d7-949a-dd15f2d3ce22"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration1, loss:6.9171\n",
            "iteration2, loss:227.2087\n",
            "iteration3, loss:6.3721\n",
            "iteration4, loss:6.8186\n",
            "iteration5, loss:6.2578\n",
            "iteration6, loss:5.9266\n",
            "iteration7, loss:3.0928\n",
            "iteration8, loss:2.9957\n",
            "iteration9, loss:3.1480\n",
            "iteration10, loss:3.6097\n",
            "iteration11, loss:3.0805\n",
            "iteration12, loss:3.0162\n",
            "iteration13, loss:3.5807\n",
            "iteration14, loss:2.8335\n",
            "iteration15, loss:3.1801\n",
            "iteration16, loss:3.1964\n",
            "iteration17, loss:2.7811\n",
            "iteration18, loss:2.6205\n",
            "iteration19, loss:2.7319\n",
            "iteration20, loss:2.6742\n",
            "iteration21, loss:3.0469\n",
            "iteration22, loss:2.6236\n",
            "iteration23, loss:2.4369\n",
            "iteration24, loss:2.7222\n",
            "iteration25, loss:2.5361\n",
            "iteration26, loss:2.4825\n",
            "iteration27, loss:2.3662\n",
            "iteration28, loss:2.3870\n",
            "iteration29, loss:2.3394\n",
            "iteration30, loss:2.4173\n",
            "iteration31, loss:2.4183\n",
            "iteration32, loss:2.3631\n",
            "iteration33, loss:2.3593\n",
            "iteration34, loss:2.3622\n",
            "iteration35, loss:2.4068\n",
            "iteration36, loss:2.3999\n",
            "iteration37, loss:2.3030\n",
            "iteration38, loss:2.3433\n",
            "iteration39, loss:2.2756\n",
            "iteration40, loss:2.3595\n",
            "iteration41, loss:2.3070\n",
            "iteration42, loss:2.3352\n",
            "iteration43, loss:2.2662\n",
            "iteration44, loss:2.3763\n",
            "iteration45, loss:2.3125\n",
            "iteration46, loss:2.2940\n",
            "iteration47, loss:2.3325\n",
            "iteration48, loss:2.3729\n",
            "iteration49, loss:2.3476\n",
            "iteration50, loss:2.3297\n",
            "iteration51, loss:2.3338\n",
            "iteration52, loss:2.3316\n",
            "iteration53, loss:2.3332\n",
            "iteration54, loss:2.3679\n",
            "iteration55, loss:2.3287\n",
            "iteration56, loss:2.3320\n",
            "iteration57, loss:2.3517\n",
            "iteration58, loss:2.3616\n",
            "iteration59, loss:2.3610\n",
            "iteration60, loss:2.3770\n",
            "iteration61, loss:2.3152\n",
            "iteration62, loss:2.3351\n",
            "iteration63, loss:2.3419\n",
            "iteration64, loss:2.3631\n",
            "iteration65, loss:2.3708\n",
            "iteration66, loss:2.2904\n",
            "iteration67, loss:2.3615\n",
            "iteration68, loss:2.3698\n",
            "iteration69, loss:2.3301\n",
            "iteration70, loss:2.3391\n",
            "iteration71, loss:2.3175\n",
            "iteration72, loss:2.3386\n",
            "iteration73, loss:2.3350\n",
            "iteration74, loss:2.3057\n",
            "iteration75, loss:2.3445\n",
            "iteration76, loss:2.3437\n",
            "iteration77, loss:2.3376\n",
            "iteration78, loss:2.2687\n",
            "iteration79, loss:2.2976\n",
            "iteration80, loss:2.3073\n",
            "iteration81, loss:2.3563\n",
            "iteration82, loss:2.3519\n",
            "iteration83, loss:2.3270\n",
            "iteration84, loss:2.3184\n",
            "iteration85, loss:2.3715\n",
            "iteration86, loss:2.3587\n",
            "iteration87, loss:2.3431\n",
            "iteration88, loss:2.3094\n",
            "iteration89, loss:2.3377\n",
            "iteration90, loss:2.2857\n",
            "iteration91, loss:2.3636\n",
            "iteration92, loss:2.2962\n",
            "iteration93, loss:2.3370\n",
            "iteration94, loss:2.3255\n",
            "iteration95, loss:2.2793\n",
            "iteration96, loss:2.3375\n",
            "iteration97, loss:2.3078\n",
            "iteration98, loss:2.3318\n",
            "iteration99, loss:2.3188\n",
            "iteration100, loss:2.2882\n",
            "iteration101, loss:2.3291\n",
            "iteration102, loss:2.2878\n",
            "iteration103, loss:2.4142\n",
            "iteration104, loss:2.3162\n",
            "iteration105, loss:2.3639\n",
            "iteration106, loss:2.3001\n",
            "iteration107, loss:2.3207\n",
            "iteration108, loss:2.3477\n",
            "iteration109, loss:2.2978\n",
            "iteration110, loss:2.3273\n",
            "iteration111, loss:2.3062\n",
            "iteration112, loss:2.3228\n",
            "iteration113, loss:2.3648\n",
            "iteration114, loss:2.3494\n",
            "iteration115, loss:2.3915\n",
            "iteration116, loss:2.3560\n",
            "iteration117, loss:2.3258\n",
            "iteration118, loss:2.3104\n",
            "iteration119, loss:2.3789\n",
            "iteration120, loss:2.3244\n",
            "iteration121, loss:2.3410\n",
            "iteration122, loss:2.2911\n",
            "iteration123, loss:2.2855\n",
            "iteration124, loss:2.3481\n",
            "iteration125, loss:2.2936\n",
            "iteration126, loss:2.3607\n",
            "iteration127, loss:2.3180\n",
            "iteration128, loss:2.3089\n",
            "iteration129, loss:2.3109\n",
            "iteration130, loss:2.3388\n",
            "iteration131, loss:2.3884\n",
            "iteration132, loss:2.3309\n",
            "iteration133, loss:2.3457\n",
            "iteration134, loss:2.3445\n",
            "iteration135, loss:2.2926\n",
            "iteration136, loss:2.3538\n",
            "iteration137, loss:2.3213\n",
            "iteration138, loss:2.3271\n",
            "iteration139, loss:2.3069\n",
            "iteration140, loss:2.3637\n",
            "iteration141, loss:2.2944\n",
            "iteration142, loss:2.3448\n",
            "iteration143, loss:2.3618\n",
            "iteration144, loss:2.3159\n",
            "iteration145, loss:2.3475\n",
            "iteration146, loss:2.3190\n",
            "iteration147, loss:2.3471\n",
            "iteration148, loss:2.3266\n",
            "iteration149, loss:2.3014\n",
            "iteration150, loss:2.3266\n",
            "iteration151, loss:2.3096\n",
            "iteration152, loss:2.3284\n",
            "iteration153, loss:2.3157\n",
            "iteration154, loss:2.3100\n",
            "iteration155, loss:2.3052\n",
            "iteration156, loss:2.3515\n",
            "iteration157, loss:2.3101\n",
            "iteration158, loss:2.3453\n",
            "iteration159, loss:2.3003\n",
            "iteration160, loss:2.3048\n",
            "iteration161, loss:2.3223\n",
            "iteration162, loss:2.3274\n",
            "iteration163, loss:2.3244\n",
            "iteration164, loss:2.3202\n",
            "iteration165, loss:2.3074\n",
            "iteration166, loss:2.3014\n",
            "iteration167, loss:2.3171\n",
            "iteration168, loss:2.2974\n",
            "iteration169, loss:2.3419\n",
            "iteration170, loss:2.3100\n",
            "iteration171, loss:2.3119\n",
            "iteration172, loss:2.2803\n",
            "iteration173, loss:2.3055\n",
            "iteration174, loss:2.3147\n",
            "iteration175, loss:2.3381\n",
            "iteration176, loss:2.3309\n",
            "iteration177, loss:2.3298\n",
            "iteration178, loss:2.3253\n",
            "iteration179, loss:2.3215\n",
            "iteration180, loss:2.3476\n",
            "iteration181, loss:2.3124\n",
            "iteration182, loss:2.3094\n",
            "iteration183, loss:2.2953\n",
            "iteration184, loss:2.2971\n",
            "iteration185, loss:2.2989\n",
            "iteration186, loss:2.2944\n",
            "iteration187, loss:2.3540\n",
            "iteration188, loss:2.2854\n",
            "iteration189, loss:2.3234\n",
            "iteration190, loss:2.2891\n",
            "iteration191, loss:2.3150\n",
            "iteration192, loss:2.3289\n",
            "iteration193, loss:2.3310\n",
            "iteration194, loss:2.3391\n",
            "iteration195, loss:2.3141\n",
            "iteration196, loss:2.3049\n",
            "iteration197, loss:2.3507\n",
            "iteration198, loss:2.3575\n",
            "iteration199, loss:2.3113\n",
            "iteration200, loss:2.2841\n",
            "iteration201, loss:2.3163\n",
            "iteration202, loss:2.3531\n",
            "iteration203, loss:2.3110\n",
            "iteration204, loss:2.3137\n",
            "iteration205, loss:2.3319\n",
            "iteration206, loss:2.3281\n",
            "iteration207, loss:2.3404\n",
            "iteration208, loss:2.2688\n",
            "iteration209, loss:2.3039\n",
            "iteration210, loss:2.2820\n",
            "iteration211, loss:2.3210\n",
            "iteration212, loss:2.3207\n",
            "iteration213, loss:2.3216\n",
            "iteration214, loss:2.3585\n",
            "iteration215, loss:2.3073\n",
            "iteration216, loss:2.3013\n",
            "iteration217, loss:2.3455\n",
            "iteration218, loss:2.2861\n",
            "iteration219, loss:2.3368\n",
            "iteration220, loss:2.2997\n",
            "iteration221, loss:2.3496\n",
            "iteration222, loss:2.3385\n",
            "iteration223, loss:2.3401\n",
            "iteration224, loss:2.3285\n",
            "iteration225, loss:2.3070\n",
            "iteration226, loss:2.3055\n",
            "iteration227, loss:2.3439\n",
            "iteration228, loss:2.3249\n",
            "iteration229, loss:2.3383\n",
            "iteration230, loss:2.3361\n",
            "iteration231, loss:2.3379\n",
            "iteration232, loss:2.3369\n",
            "iteration233, loss:2.3630\n",
            "iteration234, loss:2.2847\n",
            "iteration235, loss:2.3360\n",
            "iteration236, loss:2.3163\n",
            "iteration237, loss:2.3018\n",
            "iteration238, loss:2.3388\n",
            "iteration239, loss:2.3352\n",
            "iteration240, loss:2.3244\n",
            "iteration241, loss:2.3481\n",
            "iteration242, loss:2.3101\n",
            "iteration243, loss:2.3098\n",
            "iteration244, loss:2.3115\n",
            "iteration245, loss:2.3246\n",
            "iteration246, loss:2.3098\n",
            "iteration247, loss:2.3117\n",
            "iteration248, loss:2.3141\n",
            "iteration249, loss:2.3315\n",
            "iteration250, loss:2.3141\n",
            "iteration251, loss:2.3183\n",
            "iteration252, loss:2.3158\n",
            "iteration253, loss:2.3187\n",
            "iteration254, loss:2.3698\n",
            "iteration255, loss:2.3152\n",
            "iteration256, loss:2.3188\n",
            "iteration257, loss:2.2985\n",
            "iteration258, loss:2.3094\n",
            "iteration259, loss:2.3160\n",
            "iteration260, loss:2.3235\n",
            "iteration261, loss:2.3289\n",
            "iteration262, loss:2.3358\n",
            "iteration263, loss:2.3080\n",
            "iteration264, loss:2.3127\n",
            "iteration265, loss:2.2954\n",
            "iteration266, loss:2.3174\n",
            "iteration267, loss:2.3198\n",
            "iteration268, loss:2.2825\n",
            "iteration269, loss:2.3129\n",
            "iteration270, loss:2.2901\n",
            "iteration271, loss:2.3380\n",
            "iteration272, loss:2.3162\n",
            "iteration273, loss:2.2960\n",
            "iteration274, loss:2.3139\n",
            "iteration275, loss:2.2971\n",
            "iteration276, loss:2.3228\n",
            "iteration277, loss:2.3044\n",
            "iteration278, loss:2.3335\n",
            "iteration279, loss:2.2839\n",
            "iteration280, loss:2.3178\n",
            "iteration281, loss:2.2779\n",
            "iteration282, loss:2.3253\n",
            "iteration283, loss:2.3155\n",
            "iteration284, loss:2.3430\n",
            "iteration285, loss:2.3319\n",
            "iteration286, loss:2.3123\n",
            "iteration287, loss:2.2955\n",
            "iteration288, loss:2.2904\n",
            "iteration289, loss:2.2987\n",
            "iteration290, loss:2.3196\n",
            "iteration291, loss:2.3196\n",
            "iteration292, loss:2.3049\n",
            "iteration293, loss:2.3046\n",
            "iteration294, loss:2.3130\n",
            "iteration295, loss:2.3283\n",
            "iteration296, loss:2.3316\n",
            "iteration297, loss:2.3395\n",
            "iteration298, loss:2.3199\n",
            "iteration299, loss:2.3311\n",
            "iteration300, loss:2.2910\n",
            "iteration301, loss:2.3441\n",
            "iteration302, loss:2.2988\n",
            "iteration303, loss:2.3071\n",
            "iteration304, loss:2.3103\n",
            "iteration305, loss:2.3072\n",
            "iteration306, loss:2.3197\n",
            "iteration307, loss:2.3430\n",
            "iteration308, loss:2.2895\n",
            "iteration309, loss:2.3482\n",
            "iteration310, loss:2.3100\n",
            "iteration311, loss:2.3299\n",
            "iteration312, loss:2.3246\n",
            "iteration313, loss:2.3144\n",
            "iteration314, loss:2.3278\n",
            "iteration315, loss:2.3304\n",
            "iteration316, loss:2.2840\n",
            "iteration317, loss:2.3317\n",
            "iteration318, loss:2.2954\n",
            "iteration319, loss:2.3304\n",
            "iteration320, loss:2.2994\n",
            "iteration321, loss:2.3636\n",
            "iteration322, loss:2.3445\n",
            "iteration323, loss:2.3021\n",
            "iteration324, loss:2.3156\n",
            "iteration325, loss:2.3173\n",
            "iteration326, loss:2.3193\n",
            "iteration327, loss:2.3175\n",
            "iteration328, loss:2.3232\n",
            "iteration329, loss:2.3122\n",
            "iteration330, loss:2.3001\n",
            "iteration331, loss:2.3204\n",
            "iteration332, loss:2.3275\n",
            "iteration333, loss:2.3068\n",
            "iteration334, loss:2.3263\n",
            "iteration335, loss:2.3099\n",
            "iteration336, loss:2.3490\n",
            "iteration337, loss:2.3208\n",
            "iteration338, loss:2.3166\n",
            "iteration339, loss:2.2846\n",
            "iteration340, loss:2.2879\n",
            "iteration341, loss:2.2947\n",
            "iteration342, loss:2.3329\n",
            "iteration343, loss:2.3175\n",
            "iteration344, loss:2.2866\n",
            "iteration345, loss:2.3095\n",
            "iteration346, loss:2.2822\n",
            "iteration347, loss:2.2978\n",
            "iteration348, loss:2.3536\n",
            "iteration349, loss:2.2844\n",
            "iteration350, loss:2.3189\n",
            "iteration351, loss:2.3035\n",
            "iteration352, loss:2.3087\n",
            "iteration353, loss:2.3310\n",
            "iteration354, loss:2.3057\n",
            "iteration355, loss:2.2810\n",
            "iteration356, loss:2.2948\n",
            "iteration357, loss:2.3340\n",
            "iteration358, loss:2.2938\n",
            "iteration359, loss:2.2967\n",
            "iteration360, loss:2.3164\n",
            "iteration361, loss:2.2889\n",
            "iteration362, loss:2.3513\n",
            "iteration363, loss:2.2983\n",
            "iteration364, loss:2.3151\n",
            "iteration365, loss:2.3760\n",
            "iteration366, loss:2.3055\n",
            "iteration367, loss:2.3102\n",
            "iteration368, loss:2.3354\n",
            "iteration369, loss:2.3098\n",
            "iteration370, loss:2.3215\n",
            "iteration371, loss:2.3104\n",
            "iteration372, loss:2.2980\n",
            "iteration373, loss:2.3006\n",
            "iteration374, loss:2.3178\n",
            "iteration375, loss:2.2997\n",
            "iteration376, loss:2.3089\n",
            "iteration377, loss:2.3031\n",
            "iteration378, loss:2.2783\n",
            "iteration379, loss:2.3002\n",
            "iteration380, loss:2.3026\n",
            "iteration381, loss:2.3067\n",
            "iteration382, loss:2.3119\n",
            "iteration383, loss:2.3338\n",
            "iteration384, loss:2.3133\n",
            "iteration385, loss:2.3218\n",
            "iteration386, loss:2.3345\n",
            "iteration387, loss:2.3106\n",
            "iteration388, loss:2.3120\n",
            "iteration389, loss:2.3382\n",
            "iteration390, loss:2.3101\n",
            "iteration391, loss:2.3060\n",
            "iteration392, loss:2.2991\n",
            "iteration393, loss:2.3053\n",
            "iteration394, loss:2.3126\n",
            "iteration395, loss:2.3355\n",
            "iteration396, loss:2.3258\n",
            "iteration397, loss:2.3087\n",
            "iteration398, loss:2.3120\n",
            "iteration399, loss:2.3012\n",
            "iteration400, loss:2.3179\n",
            "iteration401, loss:2.2965\n",
            "iteration402, loss:2.3113\n",
            "iteration403, loss:2.3004\n",
            "iteration404, loss:2.3076\n",
            "iteration405, loss:2.3066\n",
            "iteration406, loss:2.3178\n",
            "iteration407, loss:2.2864\n",
            "iteration408, loss:2.3412\n",
            "iteration409, loss:2.3416\n",
            "iteration410, loss:2.3196\n",
            "iteration411, loss:2.3294\n",
            "iteration412, loss:2.2797\n",
            "iteration413, loss:2.3153\n",
            "iteration414, loss:2.3133\n",
            "iteration415, loss:2.3030\n",
            "iteration416, loss:2.3265\n",
            "iteration417, loss:2.2982\n",
            "iteration418, loss:2.3229\n",
            "iteration419, loss:2.2839\n",
            "iteration420, loss:2.3460\n",
            "iteration421, loss:2.3504\n",
            "iteration422, loss:2.3368\n",
            "iteration423, loss:2.3101\n",
            "iteration424, loss:2.3219\n",
            "iteration425, loss:2.3251\n",
            "iteration426, loss:2.2906\n",
            "iteration427, loss:2.3137\n",
            "iteration428, loss:2.3137\n",
            "iteration429, loss:2.2975\n",
            "iteration430, loss:2.3045\n",
            "iteration431, loss:2.3096\n",
            "iteration432, loss:2.3328\n",
            "iteration433, loss:2.2878\n",
            "iteration434, loss:2.3259\n",
            "iteration435, loss:2.3048\n",
            "iteration436, loss:2.2879\n",
            "iteration437, loss:2.3218\n",
            "iteration438, loss:2.3578\n",
            "iteration439, loss:2.3057\n",
            "iteration440, loss:2.3216\n",
            "iteration441, loss:2.3066\n",
            "iteration442, loss:2.2882\n",
            "iteration443, loss:2.3286\n",
            "iteration444, loss:2.3230\n",
            "iteration445, loss:2.3126\n",
            "iteration446, loss:2.2888\n",
            "iteration447, loss:2.3160\n",
            "iteration448, loss:2.3420\n",
            "iteration449, loss:2.3365\n",
            "iteration450, loss:2.3217\n",
            "iteration451, loss:2.2904\n",
            "iteration452, loss:2.3140\n",
            "iteration453, loss:2.3329\n",
            "iteration454, loss:2.3105\n",
            "iteration455, loss:2.2875\n",
            "iteration456, loss:2.2870\n",
            "iteration457, loss:2.2818\n",
            "iteration458, loss:2.3227\n",
            "iteration459, loss:2.3307\n",
            "iteration460, loss:2.3021\n",
            "iteration461, loss:2.3086\n",
            "iteration462, loss:2.3180\n",
            "iteration463, loss:2.3330\n",
            "iteration464, loss:2.3378\n",
            "iteration465, loss:2.3062\n",
            "iteration466, loss:2.3139\n",
            "iteration467, loss:2.3101\n",
            "iteration468, loss:2.3026\n",
            "iteration469, loss:2.3012\n",
            "iteration470, loss:2.2978\n",
            "iteration471, loss:2.3185\n",
            "iteration472, loss:2.3008\n",
            "iteration473, loss:2.3658\n",
            "iteration474, loss:2.2776\n",
            "iteration475, loss:2.3150\n",
            "iteration476, loss:2.2993\n",
            "iteration477, loss:2.2920\n",
            "iteration478, loss:2.2671\n",
            "iteration479, loss:2.1670\n",
            "iteration480, loss:2.2288\n",
            "iteration481, loss:2.0834\n",
            "iteration482, loss:2.4284\n",
            "iteration483, loss:5.3028\n",
            "iteration484, loss:2.1072\n",
            "iteration485, loss:2.6126\n",
            "iteration486, loss:2.3744\n",
            "iteration487, loss:2.2887\n",
            "iteration488, loss:2.4470\n",
            "iteration489, loss:2.4421\n",
            "iteration490, loss:2.4005\n",
            "iteration491, loss:2.2986\n",
            "iteration492, loss:2.5922\n",
            "iteration493, loss:2.2935\n",
            "iteration494, loss:2.5514\n",
            "iteration495, loss:2.4416\n",
            "iteration496, loss:2.5075\n",
            "iteration497, loss:2.5934\n",
            "iteration498, loss:2.3457\n",
            "iteration499, loss:2.3230\n",
            "iteration500, loss:2.4681\n",
            "iteration501, loss:2.4428\n",
            "iteration502, loss:2.4369\n",
            "iteration503, loss:2.3239\n",
            "iteration504, loss:2.3295\n",
            "iteration505, loss:2.3499\n",
            "iteration506, loss:2.2807\n",
            "iteration507, loss:2.3828\n",
            "iteration508, loss:2.6811\n",
            "iteration509, loss:2.2956\n",
            "iteration510, loss:2.3077\n",
            "iteration511, loss:2.3832\n",
            "iteration512, loss:2.3462\n",
            "iteration513, loss:2.3749\n",
            "iteration514, loss:2.2774\n",
            "iteration515, loss:2.3060\n",
            "iteration516, loss:2.3527\n",
            "iteration517, loss:2.3272\n",
            "iteration518, loss:2.3400\n",
            "iteration519, loss:2.3127\n",
            "iteration520, loss:2.2933\n",
            "iteration521, loss:2.3275\n",
            "iteration522, loss:2.3067\n",
            "iteration523, loss:2.2744\n",
            "iteration524, loss:2.4659\n",
            "iteration525, loss:2.3203\n",
            "iteration526, loss:2.3254\n",
            "iteration527, loss:2.3365\n",
            "iteration528, loss:2.3301\n",
            "iteration529, loss:2.3244\n",
            "iteration530, loss:2.3095\n",
            "iteration531, loss:2.2941\n",
            "iteration532, loss:2.3394\n",
            "iteration533, loss:2.3510\n",
            "iteration534, loss:2.3417\n",
            "iteration535, loss:2.3480\n",
            "iteration536, loss:2.3231\n",
            "iteration537, loss:2.3017\n",
            "iteration538, loss:2.3150\n",
            "iteration539, loss:2.3242\n",
            "iteration540, loss:2.3251\n",
            "iteration541, loss:2.3220\n",
            "iteration542, loss:2.3053\n",
            "iteration543, loss:2.3104\n",
            "iteration544, loss:2.3246\n",
            "iteration545, loss:2.3127\n",
            "iteration546, loss:2.3117\n",
            "iteration547, loss:2.3394\n",
            "iteration548, loss:2.3031\n",
            "iteration549, loss:2.3299\n",
            "iteration550, loss:2.3051\n",
            "iteration551, loss:2.3139\n",
            "iteration552, loss:2.3050\n",
            "iteration553, loss:2.3050\n",
            "iteration554, loss:2.3163\n",
            "iteration555, loss:2.3017\n",
            "iteration556, loss:2.3434\n",
            "iteration557, loss:2.2974\n",
            "iteration558, loss:2.3112\n",
            "iteration559, loss:2.3449\n",
            "iteration560, loss:2.3504\n",
            "iteration561, loss:2.3086\n",
            "iteration562, loss:2.3424\n",
            "iteration563, loss:2.3228\n",
            "iteration564, loss:2.3053\n",
            "iteration565, loss:2.3089\n",
            "iteration566, loss:2.3076\n",
            "iteration567, loss:2.3127\n",
            "iteration568, loss:2.3129\n",
            "iteration569, loss:2.3275\n",
            "iteration570, loss:2.3540\n",
            "iteration571, loss:2.3298\n",
            "iteration572, loss:2.3178\n",
            "iteration573, loss:2.3366\n",
            "iteration574, loss:2.3288\n",
            "iteration575, loss:2.3053\n",
            "iteration576, loss:2.3029\n",
            "iteration577, loss:2.3108\n",
            "iteration578, loss:2.2924\n",
            "iteration579, loss:2.3189\n",
            "iteration580, loss:2.2945\n",
            "iteration581, loss:2.3065\n",
            "iteration582, loss:2.2996\n",
            "iteration583, loss:2.2935\n",
            "iteration584, loss:2.3195\n",
            "iteration585, loss:2.3187\n",
            "iteration586, loss:2.2924\n",
            "iteration587, loss:2.3119\n",
            "iteration588, loss:2.2939\n",
            "iteration589, loss:2.3277\n",
            "iteration590, loss:2.3010\n",
            "iteration591, loss:2.3086\n",
            "iteration592, loss:2.3213\n",
            "iteration593, loss:2.3144\n",
            "iteration594, loss:2.3080\n",
            "iteration595, loss:2.3188\n",
            "iteration596, loss:2.3038\n",
            "iteration597, loss:2.3209\n",
            "iteration598, loss:2.3258\n",
            "iteration599, loss:2.3049\n",
            "iteration600, loss:2.3081\n",
            "iteration601, loss:2.3275\n",
            "iteration602, loss:2.3158\n",
            "iteration603, loss:2.3181\n",
            "iteration604, loss:2.2930\n",
            "iteration605, loss:2.3119\n",
            "iteration606, loss:2.3111\n",
            "iteration607, loss:2.3007\n",
            "iteration608, loss:2.3139\n",
            "iteration609, loss:2.3319\n",
            "iteration610, loss:2.2863\n",
            "iteration611, loss:2.3137\n",
            "iteration612, loss:2.3013\n",
            "iteration613, loss:2.3151\n",
            "iteration614, loss:2.3324\n",
            "iteration615, loss:2.3405\n",
            "iteration616, loss:2.3234\n",
            "iteration617, loss:2.2895\n",
            "iteration618, loss:2.3271\n",
            "iteration619, loss:2.2788\n",
            "iteration620, loss:2.3138\n",
            "iteration621, loss:2.2820\n",
            "iteration622, loss:2.3229\n",
            "iteration623, loss:2.3034\n",
            "iteration624, loss:2.3413\n",
            "iteration625, loss:2.3067\n",
            "iteration626, loss:2.3229\n",
            "iteration627, loss:2.3165\n",
            "iteration628, loss:2.3044\n",
            "iteration629, loss:2.3114\n",
            "iteration630, loss:2.3145\n",
            "iteration631, loss:2.3142\n",
            "iteration632, loss:2.3198\n",
            "iteration633, loss:2.3126\n",
            "iteration634, loss:2.3034\n",
            "iteration635, loss:2.3009\n",
            "iteration636, loss:2.3305\n",
            "iteration637, loss:2.2845\n",
            "iteration638, loss:2.3168\n",
            "iteration639, loss:2.3069\n",
            "iteration640, loss:2.3118\n",
            "iteration641, loss:2.3270\n",
            "iteration642, loss:2.2929\n",
            "iteration643, loss:2.3243\n",
            "iteration644, loss:2.3050\n",
            "iteration645, loss:2.3090\n",
            "iteration646, loss:2.2917\n",
            "iteration647, loss:2.3152\n",
            "iteration648, loss:2.2884\n",
            "iteration649, loss:2.3144\n",
            "iteration650, loss:2.3429\n",
            "iteration651, loss:2.3069\n",
            "iteration652, loss:2.3280\n",
            "iteration653, loss:2.3105\n",
            "iteration654, loss:2.3100\n",
            "iteration655, loss:2.3126\n",
            "iteration656, loss:2.3240\n",
            "iteration657, loss:2.3178\n",
            "iteration658, loss:2.2945\n",
            "iteration659, loss:2.2996\n",
            "iteration660, loss:2.2957\n",
            "iteration661, loss:2.2837\n",
            "iteration662, loss:2.3519\n",
            "iteration663, loss:2.3047\n",
            "iteration664, loss:2.3147\n",
            "iteration665, loss:2.2864\n",
            "iteration666, loss:2.2839\n",
            "iteration667, loss:2.2926\n",
            "iteration668, loss:2.3133\n",
            "iteration669, loss:2.3358\n",
            "iteration670, loss:2.3080\n",
            "iteration671, loss:2.2999\n",
            "iteration672, loss:2.3395\n",
            "iteration673, loss:2.3238\n",
            "iteration674, loss:2.2903\n",
            "iteration675, loss:2.3094\n",
            "iteration676, loss:2.3153\n",
            "iteration677, loss:2.3073\n",
            "iteration678, loss:2.3408\n",
            "iteration679, loss:2.3259\n",
            "iteration680, loss:2.3275\n",
            "iteration681, loss:2.3474\n",
            "iteration682, loss:2.3339\n",
            "iteration683, loss:2.2917\n",
            "iteration684, loss:2.3040\n",
            "iteration685, loss:2.3083\n",
            "iteration686, loss:2.3105\n",
            "iteration687, loss:2.3102\n",
            "iteration688, loss:2.3015\n",
            "iteration689, loss:2.2939\n",
            "iteration690, loss:2.3191\n",
            "iteration691, loss:2.3137\n",
            "iteration692, loss:2.2930\n",
            "iteration693, loss:2.3162\n",
            "iteration694, loss:2.3188\n",
            "iteration695, loss:2.3041\n",
            "iteration696, loss:2.3133\n",
            "iteration697, loss:2.2988\n",
            "iteration698, loss:2.3429\n",
            "iteration699, loss:2.3342\n",
            "iteration700, loss:2.3249\n",
            "iteration701, loss:2.3269\n",
            "iteration702, loss:2.3318\n",
            "iteration703, loss:2.3088\n",
            "iteration704, loss:2.3133\n",
            "iteration705, loss:2.2993\n",
            "iteration706, loss:2.3087\n",
            "iteration707, loss:2.3032\n",
            "iteration708, loss:2.2839\n",
            "iteration709, loss:2.2929\n",
            "iteration710, loss:2.3058\n",
            "iteration711, loss:2.3129\n",
            "iteration712, loss:2.2954\n",
            "iteration713, loss:2.2927\n",
            "iteration714, loss:2.3122\n",
            "iteration715, loss:2.3095\n",
            "iteration716, loss:2.3120\n",
            "iteration717, loss:2.2947\n",
            "iteration718, loss:2.3262\n",
            "iteration719, loss:2.3204\n",
            "iteration720, loss:2.3263\n",
            "iteration721, loss:2.2971\n",
            "iteration722, loss:2.3044\n",
            "iteration723, loss:2.3142\n",
            "iteration724, loss:2.2911\n",
            "iteration725, loss:2.3061\n",
            "iteration726, loss:2.3125\n",
            "iteration727, loss:2.3177\n",
            "iteration728, loss:2.3296\n",
            "iteration729, loss:2.3207\n",
            "iteration730, loss:2.2948\n",
            "iteration731, loss:2.2839\n",
            "iteration732, loss:2.3327\n",
            "iteration733, loss:2.3087\n",
            "iteration734, loss:2.3005\n",
            "iteration735, loss:2.3214\n",
            "iteration736, loss:2.3157\n",
            "iteration737, loss:2.3093\n",
            "iteration738, loss:2.3061\n",
            "iteration739, loss:2.3163\n",
            "iteration740, loss:2.3130\n",
            "iteration741, loss:2.3056\n",
            "iteration742, loss:2.2907\n",
            "iteration743, loss:2.2915\n",
            "iteration744, loss:2.3028\n",
            "iteration745, loss:2.3135\n",
            "iteration746, loss:2.2986\n",
            "iteration747, loss:2.2948\n",
            "iteration748, loss:2.2986\n",
            "iteration749, loss:2.2992\n",
            "iteration750, loss:2.3136\n",
            "iteration751, loss:2.3201\n",
            "iteration752, loss:2.3039\n",
            "iteration753, loss:2.3134\n",
            "iteration754, loss:2.3077\n",
            "iteration755, loss:2.3025\n",
            "iteration756, loss:2.2928\n",
            "iteration757, loss:2.3067\n",
            "iteration758, loss:2.2882\n",
            "iteration759, loss:2.3216\n",
            "iteration760, loss:2.3040\n",
            "iteration761, loss:2.3186\n",
            "iteration762, loss:2.3127\n",
            "iteration763, loss:2.3275\n",
            "iteration764, loss:2.3078\n",
            "iteration765, loss:2.3143\n",
            "iteration766, loss:2.3069\n",
            "iteration767, loss:2.2998\n",
            "iteration768, loss:2.2911\n",
            "iteration769, loss:2.3185\n",
            "iteration770, loss:2.2684\n",
            "iteration771, loss:2.3026\n",
            "iteration772, loss:2.3092\n",
            "iteration773, loss:2.3198\n",
            "iteration774, loss:2.3260\n",
            "iteration775, loss:2.3090\n",
            "iteration776, loss:2.2984\n",
            "iteration777, loss:2.2999\n",
            "iteration778, loss:2.3277\n",
            "iteration779, loss:2.3048\n",
            "iteration780, loss:2.3046\n",
            "iteration781, loss:2.2956\n",
            "iteration782, loss:2.3019\n",
            "iteration783, loss:2.3195\n",
            "iteration784, loss:2.3028\n",
            "iteration785, loss:2.3031\n",
            "iteration786, loss:2.3137\n",
            "iteration787, loss:2.3177\n",
            "iteration788, loss:2.2934\n",
            "iteration789, loss:2.3029\n",
            "iteration790, loss:2.3001\n",
            "iteration791, loss:2.3230\n",
            "iteration792, loss:2.3099\n",
            "iteration793, loss:2.3010\n",
            "iteration794, loss:2.3106\n",
            "iteration795, loss:2.3090\n",
            "iteration796, loss:2.3043\n",
            "iteration797, loss:2.3251\n",
            "iteration798, loss:2.3220\n",
            "iteration799, loss:2.3115\n",
            "iteration800, loss:2.3305\n",
            "iteration801, loss:2.2969\n",
            "iteration802, loss:2.3013\n",
            "iteration803, loss:2.3064\n",
            "iteration804, loss:2.3162\n",
            "iteration805, loss:2.3065\n",
            "iteration806, loss:2.2787\n",
            "iteration807, loss:2.3237\n",
            "iteration808, loss:2.3147\n",
            "iteration809, loss:2.3210\n",
            "iteration810, loss:2.3223\n",
            "iteration811, loss:2.3077\n",
            "iteration812, loss:2.2907\n",
            "iteration813, loss:2.3188\n",
            "iteration814, loss:2.3088\n",
            "iteration815, loss:2.3088\n",
            "iteration816, loss:2.2836\n",
            "iteration817, loss:2.3164\n",
            "iteration818, loss:2.2911\n",
            "iteration819, loss:2.3058\n",
            "iteration820, loss:2.2954\n",
            "iteration821, loss:2.3123\n",
            "iteration822, loss:2.3296\n",
            "iteration823, loss:2.3062\n",
            "iteration824, loss:2.3093\n",
            "iteration825, loss:2.2956\n",
            "iteration826, loss:2.2956\n",
            "iteration827, loss:2.3107\n",
            "iteration828, loss:2.3212\n",
            "iteration829, loss:2.3283\n",
            "iteration830, loss:2.3205\n",
            "iteration831, loss:2.3351\n",
            "iteration832, loss:2.3144\n",
            "iteration833, loss:2.3051\n",
            "iteration834, loss:2.3193\n",
            "iteration835, loss:2.3193\n",
            "iteration836, loss:2.3120\n",
            "iteration837, loss:2.3131\n",
            "iteration838, loss:2.3037\n",
            "iteration839, loss:2.2869\n",
            "iteration840, loss:2.3098\n",
            "iteration841, loss:2.3135\n",
            "iteration842, loss:2.3253\n",
            "iteration843, loss:2.3081\n",
            "iteration844, loss:2.3121\n",
            "iteration845, loss:2.3098\n",
            "iteration846, loss:2.3024\n",
            "iteration847, loss:2.2866\n",
            "iteration848, loss:2.3064\n",
            "iteration849, loss:2.3005\n",
            "iteration850, loss:2.3057\n",
            "iteration851, loss:2.2976\n",
            "iteration852, loss:2.3169\n",
            "iteration853, loss:2.3023\n",
            "iteration854, loss:2.3201\n",
            "iteration855, loss:2.3076\n",
            "iteration856, loss:2.3334\n",
            "iteration857, loss:2.3114\n",
            "iteration858, loss:2.3324\n",
            "iteration859, loss:2.3324\n",
            "iteration860, loss:2.3200\n",
            "iteration861, loss:2.3096\n",
            "iteration862, loss:2.2987\n",
            "iteration863, loss:2.3137\n",
            "iteration864, loss:2.3143\n",
            "iteration865, loss:2.2991\n",
            "iteration866, loss:2.2990\n",
            "iteration867, loss:2.3030\n",
            "iteration868, loss:2.3050\n",
            "iteration869, loss:2.2954\n",
            "iteration870, loss:2.3125\n",
            "iteration871, loss:2.3213\n",
            "iteration872, loss:2.2948\n",
            "iteration873, loss:2.3166\n",
            "iteration874, loss:2.2930\n",
            "iteration875, loss:2.3129\n",
            "iteration876, loss:2.3095\n",
            "iteration877, loss:2.2789\n",
            "iteration878, loss:2.3244\n",
            "iteration879, loss:2.3177\n",
            "iteration880, loss:2.3315\n",
            "iteration881, loss:2.3164\n",
            "iteration882, loss:2.3140\n",
            "iteration883, loss:2.3141\n",
            "iteration884, loss:2.3344\n",
            "iteration885, loss:2.2903\n",
            "iteration886, loss:2.2905\n",
            "iteration887, loss:2.3178\n",
            "iteration888, loss:2.3035\n",
            "iteration889, loss:2.3194\n",
            "iteration890, loss:2.3191\n",
            "iteration891, loss:2.3150\n",
            "iteration892, loss:2.3162\n",
            "iteration893, loss:2.3022\n",
            "iteration894, loss:2.3103\n",
            "iteration895, loss:2.2964\n",
            "iteration896, loss:2.3094\n",
            "iteration897, loss:2.3234\n",
            "iteration898, loss:2.3287\n",
            "iteration899, loss:2.2922\n",
            "iteration900, loss:2.2976\n",
            "iteration901, loss:2.3217\n",
            "iteration902, loss:2.3121\n",
            "iteration903, loss:2.2997\n",
            "iteration904, loss:2.3082\n",
            "iteration905, loss:2.3058\n",
            "iteration906, loss:2.3124\n",
            "iteration907, loss:2.2911\n",
            "iteration908, loss:2.2972\n",
            "iteration909, loss:2.3144\n",
            "iteration910, loss:2.3068\n",
            "iteration911, loss:2.3064\n",
            "iteration912, loss:2.3098\n",
            "iteration913, loss:2.3229\n",
            "iteration914, loss:2.3318\n",
            "iteration915, loss:2.3047\n",
            "iteration916, loss:2.3071\n",
            "iteration917, loss:2.3007\n",
            "iteration918, loss:2.3034\n",
            "iteration919, loss:2.3071\n",
            "iteration920, loss:2.3027\n",
            "iteration921, loss:2.3303\n",
            "iteration922, loss:2.3237\n",
            "iteration923, loss:2.3065\n",
            "iteration924, loss:2.3114\n",
            "iteration925, loss:2.3113\n",
            "iteration926, loss:2.3153\n",
            "iteration927, loss:2.3039\n",
            "iteration928, loss:2.2925\n",
            "iteration929, loss:2.2978\n",
            "iteration930, loss:2.3133\n",
            "iteration931, loss:2.2949\n",
            "iteration932, loss:2.3008\n",
            "iteration933, loss:2.3019\n",
            "iteration934, loss:2.2953\n",
            "iteration935, loss:2.2996\n",
            "iteration936, loss:2.3181\n",
            "iteration937, loss:2.3341\n",
            "iteration938, loss:2.2645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 测试\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "for data in test_loader:\n",
        "  images, labels = data\n",
        "  images, labels = images.to(device), labels.to(device)\n",
        "  out = model(images)\n",
        "  _, predicted = torch.max(out.data, 1)\n",
        "  total = total + labels.size(0)\n",
        "  correct = correct + (predicted == labels).sum().item()\n",
        "\n",
        "print('测试图像正确率:{:.4f}%'.format(100*correct/total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "BosxUhaZmBuf",
        "outputId": "06dafd94-000d-41cb-fbca-9b6e5cb33c28"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-df5849ad8ae0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-36b59adc4086>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/vgg.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 11.17 GiB total capacity; 10.57 GiB already allocated; 18.81 MiB free; 10.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    }
  ]
}