{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2883e2fd",
   "metadata": {},
   "source": [
    "# 几个题目"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb7725c",
   "metadata": {},
   "source": [
    "## 1.What is autoencoder?\n",
    "+ 自编码器，简称AE，结合神经网络构成一套具有自动编码功能的模型\n",
    "+ 编码的本质是将信息从一种格式/形式转为另一种格式/形式的过程，目的是获取转换后格式/形式的优良特性。比如节约空间，方便相似度计算。\n",
    "+ AutoEncoder是无监督学习，即学习过程不需要使用样本的label，本质上是把样本的输入同时作为神经网络的输入和输出，通过最小化重构误差希望学习到样本的抽象特征表示Vector。\n",
    "+ 通过AutoEncoder可以对高维数据进行高效的特征提取和表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff07044c",
   "metadata": {},
   "source": [
    "## 2.What are the differences between greedy search and beam search?\n",
    "greedy search：使用softmax取出概率最大的一个结果。</br>\n",
    "beam search：beam是束宽度，每一步从概率排序的前beam个结果中随机取出一个作为最终结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbe0d32",
   "metadata": {},
   "source": [
    "## 3.What is the intuition of attention mechanism?\n",
    "+ 2015年提出，让Encoder编码出的c向量跟Decoder解码过程中的每一个输出进行加权运算，在Decoder的每一个过程中调整权重取到不一样的c向量。</br>\n",
    "+ Attention机制就是让Encoder编码出来的向量根据Decoder要解码的内容动态变化，类似人的视觉在看某一个物体的焦点，也就是对于重要部分设置更高权重\n",
    "+ 假设Encoder每个隐藏状态为$h_j$，序列长度为T，那么在第i个时刻c向量计算为</br>\n",
    "</br>$c_i = \\sum^{T}_{j=1}{a_{ij}}{h_j}$ </br>\n",
    "</br>其中，</br>\n",
    "</br>$a_{ij} = softmax(e(s_{i-1},h_j))$</br>\n",
    "</br>是Attention分数,j=1,2,...,T时刻的一个概率分布，用softmax进行计算，$s_{i-1}$ 表示Decoder在i-1时刻的状态，e为距离函数，比如使用内积。</br>\n",
    "</br>下一时刻的隐藏状态</br>\n",
    "</br>$s_i = f(s_{i-1}, y_{i-1}, c_i)$</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadcee9f",
   "metadata": {},
   "source": [
    "# 中英文自动翻译模型的构建（使用encoder-decoder模型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ce2f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bed73412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设备设置\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21875f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {0:\"<SOS>\", 1:\"<EOS>\", -1:\"<UNK>\"}\n",
    "        self.idx = 2 # 当前长度\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "            \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split():\n",
    "            self.add_word(word)\n",
    "\n",
    "    # 得到某个词的id\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return -1\n",
    "        return self.word2idx[word]\n",
    "    \n",
    "    # vocabulary的容量\n",
    "    def __len__(self):\n",
    "        return self.idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd035a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder-decoder模型的搭建\n",
    "# 1，模型训练    \n",
    "# 2，模型推理     \n",
    "# 3，模型预测，展示结果\n",
    "class EncoderRNN(nn.Module):\n",
    "    # 在构造函数内定义Embedding层和GRU层\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # input_size代表输入语言的所有单词的数量，hidden_size是GRU网络的隐藏层节点数\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "    # 前向传播\n",
    "    def forward(self, input, hidden):\n",
    "        # seq_len=1, batch=1\n",
    "        embedded = self.embedding(input).view(1, 1, self.hidden_size)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return hidden\n",
    "        \n",
    "    # 最终执行函数\n",
    "    def sample(self, seq_list):\n",
    "        word_inds = torch.LongTensor(seq_list).to(device)\n",
    "        h = self.initHidden()\n",
    "        for word_tensor in word_inds:\n",
    "            h = self(word_tensor, h)\n",
    "        return h\n",
    "        \n",
    "    # 初始化第一层h0，随机生成一个\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1448e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.maxlen = 10\n",
    "        # output_size 是输出语言的所有单词的数量，hidden_size是GRU网络的隐藏层节点数\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        # Linear作用是将前面的GRU的输出结果变成目标语言的单词的长度\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    # 前向传播\n",
    "    def forward(self, seq_input, hidden):\n",
    "        output = self.embedding(seq_input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "    \n",
    "    # 最终执行函数\n",
    "    def sample(self, pre_hidden):\n",
    "        inputs = torch.tensor([SOS_token], device=device)\n",
    "        hidden = pre_hidden\n",
    "        res = [SOS_token]\n",
    "        # 循环编码\n",
    "        for i in range(self.maxlen):\n",
    "            output, hidden = self(inputs, hidden)\n",
    "            # 需要获取最大索引作为生成单词的id\n",
    "            topv, topi = output.topk(1) # value, index\n",
    "            # 遇到句子结束符，解码结束\n",
    "            if topi.item() == EOS_token:\n",
    "                res.append(EOS_token)\n",
    "                break\n",
    "            else:\n",
    "                res.append(topi.item())\n",
    "            # 将生成的单词作为下一时刻的输入，squeeze()去掉维度为1的维度，detach保证梯度不传导   \n",
    "            inputs = topi.squeeze().detach()\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ba5c360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理句子，将句子转换成Tensor\n",
    "def sentence2tensor(lang, sentence):\n",
    "    indexes = [lang(word) for word in sentence.split()]\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "# 将(input, target)的pair都转换成Tensor\n",
    "def pair2tensor(pair):\n",
    "    input_tensor = sentence2tensor(lan1, pair[0])\n",
    "    target_tensor = sentence2tensor(lan2, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d8ec436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "# 定义句子和Vocabulary类\n",
    "lan1 = Vocabulary()\n",
    "lan2 = Vocabulary()\n",
    "# 准备数据集\n",
    "data = [['Hi.','嗨。'],\n",
    "    ['Hi.', '你 好。'],\n",
    "    ['Run.', '跑。'],\n",
    "    ['Wait!', '等等！'],\n",
    "    ['Hello!','你好。'],\n",
    "    ['I try.','让 我 来。'],\n",
    "    ['I won!','我 赢了！'],\n",
    "    ['I am ok.', '我 很好。']\n",
    "]\n",
    "\n",
    "for i, j in data:\n",
    "    lan1.add_sentence(i)\n",
    "    lan2.add_sentence(j)\n",
    "print(len(lan1))\n",
    "print(len(lan2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84095b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# 定义参数\n",
    "learning_rate = 0.001\n",
    "hidden_size = 256\n",
    "\n",
    "encoder = EncoderRNN(len(lan1), hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size, len(lan2)).to(device)\n",
    "# 网络参数\n",
    "params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(params, lr=learning_rate)\n",
    "loss = 0\n",
    "criterion = nn.NLLLoss() # Negative log likelihood Loss\n",
    "# 一共训练多少轮\n",
    "turns = 2000\n",
    "print_every = 200\n",
    "print_loss_total = 0\n",
    "# 创建随机数据\n",
    "training_pairs = [pair2tensor(random.choice(data)) for pair in range(turns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbb74669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.4897\n",
      "loss:0.1753\n",
      "loss:0.1926\n",
      "loss:0.1533\n",
      "loss:0.1449\n",
      "loss:0.1563\n",
      "loss:0.1570\n",
      "loss:0.1473\n",
      "loss:0.1426\n",
      "loss:0.1599\n"
     ]
    }
   ],
   "source": [
    "# 训练过程\n",
    "for turn in range(turns):\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    x, y = training_pairs[turn]\n",
    "    input_length = x.size(0)\n",
    "    target_length = y.size(0)\n",
    "    # 初始化Encoder中的h0\n",
    "    h = encoder.initHidden()\n",
    "    # 对input进行Encoder\n",
    "    for i in range(input_length):\n",
    "        h = encoder(x[i], h)\n",
    "    # Decoder的一个input\n",
    "    decoder_input = torch.LongTensor([SOS_token]).to(device)\n",
    "    \n",
    "    for i in range(target_length):\n",
    "        decoder_output, h = decoder(decoder_input, h)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "        loss = loss + criterion(decoder_output, y[i])\n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break\n",
    "    print_loss_total = print_loss_total + loss.item() / target_length\n",
    "    if (turn+1) % print_every == 0:\n",
    "        print('loss:{:.4f}'.format(print_loss_total/print_every))\n",
    "        print_loss_total = 0\n",
    "        \n",
    "    #optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92cab2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<SOS> 你 <EOS>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试\n",
    "def translate(s):\n",
    "    t = [lan1(i) for i in s.split()]\n",
    "    t.append(EOS_token)\n",
    "    f = encoder.sample(t)\n",
    "    s = decoder.sample(f)\n",
    "    r = [lan2.idx2word[i] for i in s]\n",
    "    return ' '.join(r) \n",
    "\n",
    "translate('Hi.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433f90d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
