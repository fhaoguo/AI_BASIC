{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词向量的过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_9k.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT = open('dataset/article_9k.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_content = CONTENT[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string): return list(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/minquan/.local/lib/python3.7/site-packages/jieba/__init__.py\", line 154, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmpz_l3l45k' -> '/tmp/jieba.cache'\n",
      "Loading model cost 0.904 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "sample_words = cut(sample_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(sample_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot 编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m one_hot_matrix \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(vocabulary)  \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mvocabulary\u001b[49m)) ]\n\u001b[1;32m      3\u001b[0m word_to_index \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      5\u001b[0m index_to_word \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocabulary' is not defined"
     ]
    }
   ],
   "source": [
    "one_hot_matrix = [[0] * len(vocabulary)  for _ in range(len(vocabulary)) ]\n",
    "\n",
    "word_to_index = {}\n",
    "\n",
    "index_to_word = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mvocabulary\u001b[49m):\n\u001b[1;32m      2\u001b[0m     one_hot_matrix[i][i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      3\u001b[0m     word_to_index[v] \u001b[38;5;241m=\u001b[39m i\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocabulary' is not defined"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(vocabulary):\n",
    "    one_hot_matrix[i][i] = 1\n",
    "    word_to_index[v] = i\n",
    "    index_to_word[i] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index['我们']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index['小米']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index['手机']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 我们要求出来 embedding的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m random_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;28mlen\u001b[39m(vocabulary), vec_dim)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "random_matrix = np.random.rand(len(vocabulary), vec_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99134138, 0.86652545, 0.08197298, ..., 0.63202863, 0.3457592 ,\n",
       "        0.49475792],\n",
       "       [0.62749232, 0.3993631 , 0.78450143, ..., 0.34577718, 0.56586633,\n",
       "        0.74254919],\n",
       "       [0.94140008, 0.57587471, 0.80235597, ..., 0.09173123, 0.83827056,\n",
       "        0.75061502],\n",
       "       ...,\n",
       "       [0.28253342, 0.56801495, 0.40835155, ..., 0.9970613 , 0.55589049,\n",
       "        0.66988305],\n",
       "       [0.39128904, 0.5108289 , 0.49379399, ..., 0.44812605, 0.75112865,\n",
       "        0.61066583],\n",
       "       [0.48895156, 0.02132593, 0.10495434, ..., 0.27503937, 0.91997329,\n",
       "        0.11361797]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_matrix[156]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([156]),)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(one_hot_matrix[156])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "xiaom_vec = np.dot(np.array(one_hot_matrix[156]), random_matrix) # 小米对应的初始化向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_vec = np.dot(np.array(one_hot_matrix[19]), random_matrix) # 手机对应的初始化向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 这两个词向量刚开始是随机的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75688353, 0.36633631, 0.80301839, 0.30416594, 0.89556025,\n",
       "       0.72090631, 0.68127117, 0.44100505, 0.5382745 , 0.77546659])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xiaom_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 然后，会初始化一个叫做hidden_layer的一个矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ xiaom_vec => (1, 10)\n",
    "+ hidden_layer => (10, len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiden_layer = np.random.rand(vec_dim, len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = np.dot(xiaom_vec, hiden_layer).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2 = np.dot(phone_vec, hiden_layer).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何output1和output2的值是相近的\n",
    "## 那么是不是说，xiaom_vec 和 phone_vec的值是相近的？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 假设，“小米”和 “手机”的意思是接近的； \n",
    "## 那么xiaom_vec 和 phone_vec 我们期望的结果应该是相似的\n",
    "## 那么，output1 和 output2 的结果应该是相似的的\n",
    "\n",
    "## 但是，现在，output1和output的结果未必是相似的\n",
    "\n",
    "##  ORACLE: 当“单词W1”和“单词W2”意思相近的时候，“output1”和“ouput2”的结果也应该相似\n",
    "\n",
    "## 我们的语言学家告诉我们，如果“w1”和“w2”的单词意思相似，那么就会有什么结果呢？w1周围的单词，和w2周围的单词，应该是类似的~ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 要把output 和它 周围的单词 链接起来，产生关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 4] #=> 一组东西的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, -1]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[3, 4, -1] #=> 一组东西的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'桃子 苹果 西瓜'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"桃子 苹果 西瓜\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    return [x / sum(X) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X -= np.max(X)\n",
    "    \n",
    "    exps = [np.exp(x) for x in X]\n",
    "    \n",
    "    return [e / sum(exps) for e in exps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_fruit_pro(x):\n",
    "    \"\"\"\n",
    "    return \n",
    "    [0, 0, 1] # 属于每个类别的概率：[属于桃子的概率是0%, 属于苹果的是0%， 属于西瓜的是100%]\n",
    "    [0, 1, 0] # 属于每个类别的概率：[属于桃子的概率是0%, 属于苹果的是100%， 属于西瓜的是0%]\n",
    "    [1, 0, 0] # 属于每个类别的概率：[属于桃子的概率是100%, 属于苹果的是0%， 属于西瓜的是0%]\n",
    "    \"\"\"\n",
    "    \n",
    "    logits = [0.99, 0.18, 0.48] # 每个类别的得分~\n",
    "    \n",
    "#    probabilities = normalize(logits)\n",
    "    \n",
    "    probabilities = softmax(logits)\n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4889130065244961, 0.2174968946337315, 0.2935900988417724]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caculate_fruit_pro([9, 1, 1, 3, 1, 3, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ cross-entropy(y_{true}, \\hat{y}) = -\\sum_i^n{y_i^{true}}{log(\\hat{y_i})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_true, y_hat):\n",
    "    return -1 * sum(y_t_i * np.log(y_hat_i) for y_t_i, y_hat_i in zip(y_true, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 假设，有一个图片，它对于的答案是桃子\n",
    "\n",
    "## 那么，我们期望的caculate_fruit_pro的结果应该是[1, 0, 0]\n",
    "## 有两个函数，一个算出来的是 [0.7, 0.2, 0.1]\n",
    "## 一个算出来的是 [0.6, 0.2, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2231435513142097"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy([1, 0, 0], [0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35667494393873245"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy([1, 0, 0], [0.7, 0.2, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5108256237659907"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy([1, 0, 0], [0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy([1, 0, 0], [0.5, 0.3, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 小米， 手机 单词 获得它的one-hot值\n",
    "+ one-hot值，获得了它们随机的 xiaom_vec, phone_vec\n",
    "+ xiaom_vec, phone_vec 和 hidden_layer做点积\n",
    "+ output1, output2\n",
    "+ output1, output2 => softmax(output1), softmax(output2)\n",
    "+ 计算cross_entropy(”小米“周围的单词的label, softmax(output1))\n",
    "+ 计算cross_entropy(“手机”周围的单词的label, softmax(output2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6， 7步的结果： 如何第一步的两个单词结果是相似的，那么，6，7步的结果也应该是相似的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 有一种叫做反向传播的机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们知道，softmax(x) 之后应该是某个值的时候，但是实际上它不是这个值，我们计算机会做更新，把x，和x相关的所有东西，进行更新，然后一次次的让\n",
    "softmax(x)的值更接近我们期望的值，这个“反向传播”，就是深度学习的核心"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词向量的过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于单词 $w_1$ 和 $w_2$ , 如果$w_1$和$w_2$的单词意思相似，那么$w_1$和$w_2$周围的单词，也应该是相似的，\n",
    "这个是语言学家告诉我们的\n",
    "\n",
    "根据$w_1$和$w_2$，我们能得到他们随机初始化的 $vec_1, vec_2$\n",
    "\n",
    "$vec_1 \\cdot hidden_m => output_1$\n",
    "\n",
    "$vec_2 \\cdot hidden_m => output_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax(output1) 赋予的含义是： $w_1$周围出现什么单词\n",
    "\n",
    "softmax(output2) 赋予的含义是： $w_2$周围出现什么单词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$loss_1$ = cross_entropy($w_1$周围真正的单词的label, softmax(output1))\n",
    "\n",
    "$loss_2$ = cross_entropy($w_2$周围真正的单词的label, softmax(output2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们期望的是，loss => 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用“反向传播”算法，能够逐渐让“loss1”和“loss2”变成0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 当loss1和loss2逐渐变成0的时候"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么这个时候，output1和output2的值也是接近的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output1和output2的值是接近的，意味着“vec_1”和“vec_2”的值也是接近的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以，得到了两个值接近的单词向量 \"vec_1\", \"vec_2\" 当 他们对应的单词 \"w1\" 和 “w2” 意思接近的时候"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
